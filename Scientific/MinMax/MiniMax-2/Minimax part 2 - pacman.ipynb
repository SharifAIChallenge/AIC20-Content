{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=red size=6>\n",
    "\t\t\t<br />\n",
    "            <img src=\"image/minmax2.jpg\" style=\"width:100%\">\n",
    "\t\t</font>\n",
    "        <font color=red size=4>\n",
    "\t\t\t<br />\n",
    "\t\t</font>\n",
    "        <br />\n",
    "\t\t<style type=\"text/css\" scoped>\n",
    "        p{\n",
    "        border: 1px solid #a2a9b1;background-color: #f8f9fa;display: inline-block;\n",
    "        };\n",
    "        </style>\n",
    "\t\t<div>\n",
    "\t\t\t<h3>فهرست مطالب</h3>\n",
    "\t\t\t<ul style=\"margin-right: 0;\">\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#intro\">\n",
    "                        مقدمه و مرور مفاهیم\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#reflex\">\n",
    "                       عامل بازتابی و تابع ارزیابی\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#Minimax\">\n",
    "                       Minimax و هرس $\\alpha - \\beta$\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#manba\">\n",
    "                       منابع\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"intro\" style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "مقدمه و مرور مفاهیم\n",
    "        </font>\n",
    "        <br>\n",
    "        در بخش قبلی با مسئلهٔ بازی‌های رقابتیِ دونفره آشنا شدیم و در مورد الگوریتم minimax برای حل این‌گونه مسائل صحبت کردیم. ایده‌ی محوری در این الگوریتم آن بود که با پیش‌بینی حرکت نفر بعدی و فرض بهینه بودن رفتار رقیب بهترین گزینهٔ ممکن را انتخاب می‌کنیم. با درپیش‌گیری همین رویکرد درخت حالات بازی را به دست می‌آوردیم و انتخاب مطلوب را انجام می‌دادیم. از سوی دیگر دیدیم که به دست آوردن درخت حالات و انجام دادن الگوریتم minimax برای اغلب بازی‌ها بسیار زمان‌بر است پس برای کاهش پیچیدگی و هزینه‌های آن با روش هرس آلفا-بتا آشنا شدیم که در آن با حذف برخی از شاخه‌ها حجم محاسبات را از نظر پیچیدگی زمانی و حافظه کاهش می‌دهد. از سوی دیگر با محدود کردن عمق و حدس زدن ارزش وضعیت‌های مختلف همین کاهش هزینه‌ها را از منظر دیگری امتحان کردیم. در کنار آشنایی با مفاهیم اصلی با بررسی دو بازی دوز و connect4 این الگوریتم‌ها را پیاده‌سازی کردیم. در ادامه قرار است دست‌هایمان را به کد آلوده کنیم و با بررسی یک مسئلهٔ پیچیده‌تر هم با مفهوم تابع ارزیابی (evaluation function) بیش‌تر آشنا می‌شویم و هم کاربردهای minimax را در بازی Pacman مرور می‌کنیم.\n",
    "        <br>\n",
    "        <img src=\"image/start.gif\" align=\"middle\" height=\"500\" width=\"500\"/>\n",
    "        <br>\n",
    "        بیایید یک بار pacman را بازی کنیم. در ژوپیتر نوت‌بوک با استفاده از دستور <code>run%</code> می‌توانیم دستورات معادل با <code>python</code> در ترمینال را اجرا کنیم. همچنین اگر به پوشهٔ layouts بروید تعدادی نقشهٔ بازی می‌بینید که با استفاده از دستور <code>run pacman.py -l layoutName%</code> آن‌ها را امتحان کنید \n",
    "    <br>\n",
    "                      با اجرای این دستور بازی pacman اجرا می‌شود و می‌توانید خودتان بازی کنید\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Pacman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pacman.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reflex\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "    درواقع Pacman یک پروژه است که در کنار فایل ژوپیتر قرار گرفته است و قرار است کدی بنویسیم که کنترل پکمن ما را به دست بگیرد\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reflex\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "        <font color=#FF7500 size=6>\n",
    "عامل بازتابی و تابع ارزیابی\n",
    "        </font>\n",
    "        <br>\n",
    "       حال می‌خواهیم برنامه‌ای بنویسیم که به پک‌من بخت‌برگشته کمک کند تا روی پای خودش بایستد و در بازی برنده شود. فعلا برای شروع کار می‌خواهیم یک عامل بازتابی (Reflex Agent) بنویسم. به طور خلاصه یک عامل بازتابی ساده در هر لحظه تمام محیط را مشاهده می‌کند و بر اساس تعدادی شرط از پیش تعیین شده رفتار می‌کند. (در پیوست این دفترچه منابعی برای آشنایی بیش‌تر با Agentها معرفی شده است.) پس قصد داریم که یک عامل بازتابی بنویسیم. برای تست کردن این عامل از دستور زیر استفاده می‌کنیم. این دستور را اجرا کنید.\n",
    "        <br>\n",
    "در واقع عامل بازتابی وظیفه کنترل پکمن ما را دارد\n",
    "   با پارمتر p AgentName - می‌توانید عامل بازتابی مورد نظر را به کد بدهید.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pacman.py -p ReflexAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reflex\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "        عامل بازتابی فوق به صورت رندوم حرکت می‌کند.\n",
    "        <br>\n",
    "می‌توانید این عامل بازتابی را در فایل multiAgents.py ببینید.\n",
    "        یا به کلاس پایین نگاه کنید.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflexAgent(Agent):\n",
    "    def getAction(self, gameState):\n",
    "        # Collect legal moves and successor states\n",
    "        return ReflexAgentActionUtil.reflexAgentAction(self, gameState)\n",
    "\n",
    "    def evaluationFunction(self, currentGameState, action):\n",
    "        return ReflexAgentEvalUtil.reflexAgentEval(self, currentGameState, action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reflex\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "کلاس بالا عامل بازتابی است و تابع که در ادامه نوشته شده است  ReflexAgentActionUtil.reflexAgentAction  هست.\n",
    "        <br>\n",
    "        همانطور که می‌بینید به ReflexAgentActionUtil.reflexAgentAction وضعیت بازی داده می‌شود و از بین وضعیت‌های ممکن یکی را به صورت تصادفی انتخاب می‌کند.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def reflexAgentAction(self, gameState):\n",
    "    legalMoves = gameState.getLegalActions()\n",
    "    return random.choice(legalMoves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reflex\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "    در واقع این کلاس اینگونه عمل می‌کند که کد اصلی پکمن برای کنترل پکمن تابع getAction این شی را صدا می‌زند و کدی که زدیم باید بگوید پکمن چه کاری انجام بدهد.\n",
    "        پکمن در هر لحظه می‌تواند به یکی از چهار جهت حرکت کند یا سرجای خود بماند.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/game_over.gif\" align=\"middle\" height=\"500\" width=\"500\"/><br>\n",
    "<div id=\"eval\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "        همان‌طور که مشاهده کردید پک‌من حرکت می‌کرد اما همیشه می‌باخت. دلیل این امر آن بود که در هر بازهٔ زمانی پک‌من از میان حرکت‌های ممکن یکی را به صورت تصادفی انتخاب می‌کند. می‌خواهیم این مشکل را به مرور رفع کنیم. پیشنهاد شما برای رفع این مشکل چیست؟ واضح است که باید سعی کنیم از میان حرکت‌های ممکن حرکتی را انجام  دهیم که بهترین باشد. اما معیار ما برای بهترین چیست؟ چه سازوکاری برای ارزیابی یک وضعیت از بازی ارائه می‌کنید؟ \n",
    "        <br>\n",
    "        بیایید فرض کنیم چنین تابعی برای ارزیابی هر حرکت و وضعیت بازی داریم. حال باید برای هر بازهٔ زمانی از بین حرکت‌های مجاز یک حرکت را انتخاب کنیم که طبعا می‌خواهیم مقدار تابع ارزیابی در این وضعیت بیشینه باشد. قطعه کد زیر این کار را انجام می‌دهد. به آن دقت کنید.\n",
    "        <br>\n",
    "        در واقع ما می‌خواهیم عامل بازتابی را تغییر بدهیم و دیدیم که این عامل برای حرکت کردن تابع reflexAgentAction از فایل ReflexAgentActionUtil.py را صدا می‌زند\n",
    "        <br>\n",
    "        پس ما این تابع را می‌خواهیم عوض کنیم.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ReflexAgentActionUtil.py\n",
    "import random\n",
    "def reflexAgentAction(reflexAgent, gameState):\n",
    "    legalMoves = gameState.getLegalActions()\n",
    "    # Choose one of the best actions\n",
    "    scores = [reflexAgent.evaluationFunction(gameState, action) for action in legalMoves]\n",
    "    bestScore = max(scores)\n",
    "    bestIndices = [index for index in range(len(scores)) if scores[index] == bestScore]\n",
    "    chosenIndex = random.choice(bestIndices) # Pick randomly among the best\n",
    "    return legalMoves[chosenIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "        حال هنگام آن است که تابع ارزیابی را کامل کنیم. یک بار دیگر به ابتدای دفترچه رفته و خودتان بازی را انجام دهید و در حین بازی به این فکر کنید که برای بردن بازی هر حرکت را بر اساس چه معیارهایی انجام می‌دهید. نکته‌ی اساسی در مورد تابع ارزیابی آن است که مفهوم پیچیده‌ای در آن نهفته نیست که ما قصد داشته باشیم در این فترچه آن را به شما آموزش دهیم بلکه بیش‌تر از هر چیز به میزان دانش شما و نحوهٔ پیاده‌سازی تابع ارزیابی برمی‌گردد.\n",
    "        <br>\n",
    "        سعی کنید تابع ارزیابی زیر را تکمیل کنید. این تابع موقعیت فعلی، یک حرکت قانونی و خود عامل را به عنوان ورودی می‌گیرد و یک عدد به عنوان میزان خوب بودن وضعیت حاصل بعد از انجام حرکت خروجی می‌دهد. چند دادهٔ مناسب که با توجه به وضعیت بازی می‌تواند مورد استفاده قرار بگیرد نیز در ابتدای این تابع ضمیمه شده است.\n",
    "        <br>\n",
    "<b>        توجه</b>: با استفاده از دستور    <code>writefile file_name.py%%</code> در واقع یک فایل را کاملا پاک می‌کنیم و از نو می‌نویسیم و لازم است برای لود شدن مجدد کرنل پایتون مورد استفاده در دفترچهٔ ژوپیتر را ری‌استارت کنیم که این کار از طریق <code>(os._exit(0</code> انجام‌پذیر است با این‌حال لازم است که عملیات تغییر پوشهٔ فعلی و لود کردن کتاب‌خانه‌ها را از طریق دستورهای <code>import os</code> یا <code>cd Pacman</code> انجام دهیم. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reflex\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "تابع reflexAgentEval سه ورودی دارد که ورودی اول خود تابع بازتابی، ورودی دوم وضعیت کنونی بازی و ورودی سوم حرکتی که می‌خواهیم انجام بدیم است.\n",
    "     و با توجه به این موارد خروجی یک امتیاز است بر اساس اینکه اگر آن حرکت را در این وضعیت بازی انجام بدیم چه امتیازی می‌گیریم.\n",
    "        <br>\n",
    "        تابع را به شکل زیر بازنویسی می‌کنیم. اما دقت کنید که خروجی این تابع فعلا همیشه ۰ است. و بعدا این تابع را بهتر می‌کنیم.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ReflexAgentEvalUtil.py\n",
    "import util\n",
    "def reflexAgentEval(reflexAgent, currentGameState, action):\n",
    "    # Useful information you can extract from a GameState (pacman.py)\n",
    "    successorGameState = currentGameState.generatePacmanSuccessor(action)\n",
    "    newPos = successorGameState.getPacmanPosition()\n",
    "    newFood = successorGameState.getFood()\n",
    "    newGhostStates = successorGameState.getGhostStates()\n",
    "    newScaredTimes = [ghostState.scaredTimer for ghostState in newGhostStates]\n",
    "    # your code here!\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Pacman\n",
    "%run pacman.py -p ReflexAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "     خب نتیجه چه بود؟ آیا در بردن بازی موفق شد؟ اگر نه باز هم تلاش کنید تا منطق بازی را به خوبی به یک تابع ارزیابی ترجمه کنید. این عامل مهمی در یک تابع ارزیابی است. مثلا به عمل‌کرد قطعه کد طولانی زیر در همین بازی دقت کنید. از آن‌جایی که به منطق بازی توجه درستی نشده عامل به جای حرکت متوالی روی نقطه‌ها بین چند وضعیت مردد می‌ماند و اگر روحی در زمین نباشد یا روح‌ها به سمت او نیایند هیچ‌گاه موفق به اتمام بازی نشود. این کد را مطالعه کنید و سعی کنید دلیل مشکلات آن را مشخص کنید.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "    کد بالا هرکدام از عامل‌های بازی را بررسی می‌کند و با توجه به آن یک امتیاز به وضعیت می‌دهد.\n",
    "        از عوامل بازی می‌شود به غذا‌ها، ارواح، ارواح ترسیده و ... توجه کرد. هر کدام از این عوامل نیز چند بخش دارند. مثلا در مورد غذا یک مورد مهم این است که فاصله ما تا آن چقدر است و یک مورد دیگر اینکه چندتا غذا خوردیم.\n",
    "        <br>\n",
    "        <font color=\"blue\" size=5>\n",
    "        برای اینکه پکمن به دنبال غذا برود باید چیکار کنیم؟ کمی به این سوال فکر کنید.\n",
    "        </font>\n",
    "        <br>\n",
    "        جواب : وقتی به دنبال غذا برود فاصله پکمن با نزدیک‌ترین غذا کم می‌شود پس اگر ما فاصله تا نزدیک‌ترین غذا را سعی کنیم کم کنیم به غذا نزدیک می‌شود.\n",
    "        <br>\n",
    "        <font color=\"blue\" size=5>\n",
    "        اما چگونه این فاصله را کم کنیم؟\n",
    "        </font>\n",
    "        <br>\n",
    "        اگر فاصله پکمن تا نزدیک ترین غذا را در امتیازدهی دخیل کنیم پکمن وضعیتی که نزدیکتر به غذا باشد را انتخاب می‌کند و به دنبال غذا می‌رود.\n",
    "        <br>\n",
    "        مثالی که زدیم یک نمونه از تبدیل یک عامل در بازی به کد است.\n",
    "        <br>\n",
    "                <font color=\"blue\" size=5>\n",
    "        سعی کنید به این فکر کنید که عوامل زیر را چگونه پیاده سازی کنیم:\n",
    "        </font>\n",
    "                    <br>\n",
    "        <ul>\n",
    "        <li>\n",
    "        خورده شدن توسط ارواح\n",
    "        </li>\n",
    "           <li>\n",
    "        خوردن ارواح اگر ترسیده اند.\n",
    "        </li>\n",
    "            <li>\n",
    "        در یک مکان گیر نکنیم که ممکن است ارواح پکمن را محاصره کنند و راه فراری نداشته باشیم.\n",
    "        </li>\n",
    "            <li>\n",
    "        خوردن غذا‌های بزرگ (که باعت می‌شود ارواح سفید شوند و بتوانیم آن‌ها را بخوریم\n",
    "        </li>\n",
    "            <li>\n",
    "        امتیاز خود بازی\n",
    "        </li>\n",
    "            </font>\n",
    "        </div>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ReflexAgentEvalUtil.py\n",
    "import util\n",
    "def reflexAgentEval(reflexAgent, currentGameState, action):\n",
    "    # Useful information you can extract from a GameState (pacman.py)\n",
    "    successorGameState = currentGameState.generatePacmanSuccessor(action)\n",
    "    newPos = successorGameState.getPacmanPosition()\n",
    "    newFood = successorGameState.getFood()\n",
    "    newGhostStates = successorGameState.getGhostStates()\n",
    "    newScaredTimes = [ghostState.scaredTimer for ghostState in newGhostStates]\n",
    "\n",
    "    def sum_food_proximity(cur_pos, food_positions, norm=False):\n",
    "        food_distances = []\n",
    "        for food in food_positions:\n",
    "            food_distances.append(util.manhattanDistance(food, cur_pos))\n",
    "        if norm:\n",
    "            return normalize(sum(food_distances) if sum(food_distances) > 0 else 1)\n",
    "        else:\n",
    "            return sum(food_distances) if sum(food_distances) > 0 else 1\n",
    "\n",
    "    score = successorGameState.getScore()\n",
    "    def ghost_stuff(cur_pos, ghost_states, radius, scores):\n",
    "        num_ghosts = 0\n",
    "        for ghost in ghost_states:\n",
    "            if util.manhattanDistance(ghost.getPosition(), cur_pos) <= radius:\n",
    "                scores -= 30\n",
    "                num_ghosts += 1\n",
    "        return scores\n",
    "\n",
    "    def food_stuff(cur_pos, food_pos, cur_score):\n",
    "        new_food = sum_food_proximity(cur_pos, food_pos)\n",
    "        cur_food = sum_food_proximity(currentGameState.getPacmanPosition(), currentGameState.getFood().asList())\n",
    "        new_food = 1/new_food\n",
    "        cur_food = 1/cur_food\n",
    "        if new_food > cur_food:\n",
    "            cur_score += (new_food - cur_food) * 3\n",
    "        else:\n",
    "            cur_score -= 20\n",
    "\n",
    "        next_food_dist = closest_dot(cur_pos, food_pos)\n",
    "        cur_food_dist = closest_dot(currentGameState.getPacmanPosition(), currentGameState.getFood().asList())\n",
    "        if next_food_dist < cur_food_dist:\n",
    "            cur_score += (next_food_dist - cur_food_dist) * 3\n",
    "        else:\n",
    "            cur_score -= 20\n",
    "        return cur_score\n",
    "\n",
    "    def closest_dot(cur_pos, food_pos):\n",
    "        food_distances = []\n",
    "        for food in food_pos:\n",
    "            food_distances.append(util.manhattanDistance(food, cur_pos))\n",
    "        return min(food_distances) if len(food_distances) > 0 else 1\n",
    "\n",
    "\n",
    "    def normalize(distance, layout):\n",
    "        return distance\n",
    "\n",
    "    return food_stuff(newPos, newFood.asList(), ghost_stuff(newPos, newGhostStates, 2, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Pacman\n",
    "%run pacman.py -p ReflexAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "در کد بالا successorGameState وضعیت بازی بعد از اجرای action است که قرار است به آن امتیاز بدهیم تا بین حالت‌های بعدی ممکن حالتی که امتیاز بیشینه دارد را انتخاب کنیم.\n",
    "        <br>\n",
    "        newPos موقعیت جدید پک‌من است.\n",
    "        <br>\n",
    "        newFood غذاهای موجود در وضعیت بعدی است.\n",
    "        <br>\n",
    "        newGhostStates وضعیت ارواح در وضعیت بعدی است.\n",
    "        <br>\n",
    "        newScaredTimes لیستی از میزان زمانی که ارواح می‌ترسند است. (وقتی پک‌من یکی از غذاهای بزرگ را می‌خورد ارواح سفید می‌شوند و در واقع ترسیده‌اند. این ترسیده بودن یک زمان محدودی دارد.\n",
    "        <br>\n",
    "       <br>\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "        حالا که با یکی دو نمونه از توابع ارزیابی دست‌وپنجه نرم کردید، به قطعه کد پایین و عملکرد آن هم نگاه کنید. می‌بینید که هنوز هم جای کار دارد و گاهی گیر می‌کند اما اغلب مواقع بازی را با پیروزی به اتمام می‌رساند. سعی کنید که عمل‌کرد این کد را با توجه به این‌که برخورد با روح‌های سفید امتیاز دارد بهبود بخشید. \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ReflexAgentEvalUtil.py\n",
    "import util\n",
    "def reflexAgentEval(reflexAgent, currentGameState, action):\n",
    "    # Useful information you can extract from a GameState (pacman.py)\n",
    "    successorGameState = currentGameState.generatePacmanSuccessor(action)\n",
    "    newPos = successorGameState.getPacmanPosition()\n",
    "    newFood = successorGameState.getFood()\n",
    "    newGhostStates = successorGameState.getGhostStates()\n",
    "    newScaredTimes = [ghostState.scaredTimer for ghostState in newGhostStates]\n",
    "\n",
    "    if successorGameState.isWin():\n",
    "        return float(\"inf\")\n",
    "\n",
    "    for ghostState in newGhostStates:\n",
    "        if util.manhattanDistance(ghostState.getPosition(), newPos) < 2 and ghostState.scaredTimer < 3:\n",
    "            return float(\"-inf\")\n",
    "\n",
    "    foodDist = []\n",
    "    for food in list(newFood.asList()):\n",
    "        foodDist.append(util.manhattanDistance(food, newPos))\n",
    "\n",
    "    foodSuccessor = 0\n",
    "    if (currentGameState.getNumFood() > successorGameState.getNumFood()):\n",
    "        foodSuccessor = 300\n",
    "\n",
    "    return successorGameState.getScore() - 5 * min(foodDist) + foodSuccessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd Pacman\n",
    "%run pacman.py -p ReflexAgent\n",
    "%run pacman.py -p ReflexAgent -l trickyClassic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "        اگر تعداد غذا‌های در موقعیت جدید کمتر باشد یعنی ما در این موقعیت یک غذا خوردیم و foodSuccessor را برابر ۳۰۰ می‌کند.\n",
    "        <br>\n",
    "        foodDist آرایه فاصله منهتنی غذا‌ها از موقعیت جدید است و اگر کمینه این فاصله‌ها کم باشد یعنی موقعیت نزدیک به یک غذا است و این خوب است. برای همین در امتیاز کمینه فاصله را از امتیاز که قرار است خروجی بدهیم کم کردیم. علت اینکه این مقدار را ضرب در پنج‌کردیم این است که تاثیر بیشتری بهش بدهیم\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Minimax\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "        <font color=#FF7500 size=6>\n",
    "         توضیحاتی برای بررسی Reflex Agent\n",
    "        </font>\n",
    "        <br>\n",
    "        برای ارواح نیز می‌توانید Reflex Agent بنویسید که جلوتر توضیح داده می‌شود. \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pacman.py -g RandomGhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Minimax\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "        <br>\n",
    "        این Reflex Agent یک حرکت را به صورت شانسی با احتمال برابر انتخاب می‌کند\n",
    "        برای ارواح Reflex Agent به صورت پیش‌فرض RandomGhost است.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pacman.py -g DirectionalGhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Minimax\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "        <br>\n",
    "        این Reflex Agent حرکتی که به پکمن نزدیک‌ترش بکند را انتخاب می‌کند.\n",
    "          در واقع به دنبال پک‌من می‌رود.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pacman.py -p ReflexAgent -q -n 5 -k 3 -f --timeout 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Minimax\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "        <br>\n",
    "        با -q گرافیک بازی نشان داده نمی‌شود که برای این است نتیجه بازی را سریع بفهمیم\n",
    "        <br>\n",
    "        با  [number of game] n- می‌توانیم تعداد بازی‌ها را مشخص کنیم\n",
    "        <br>\n",
    "        با  [number of ghosts] k- می‌توانیم تعداد ارواح را مشخص کنیم.\n",
    "        <br>\n",
    "        با f- می‌توانیم بگوییم که از یک seed مشخص برای رندوم استفاده کند\n",
    "        <br>\n",
    "        با timeout-- می‌توانید ماکسیمم زمانی که agent می‌تواند برا محاسبه انجام بدهد را تعیین کنید\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Minimax\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "        <font color=#FF7500 size=6>\n",
    "عامل Minimax و $\\alpha-\\beta$\n",
    "        </font>\n",
    "        <br>\n",
    "         در بخش قبلی یک Reflex Agent برای بازی پک‌من نوشتیم. اما توجه کنید در آن حالت تمامی اطلاعات بازی را داشتیم و می‌دانستیم که هر روح چه انتخابی انجام می‌دهد و کجا می‌رود و این را در تصمیم‌گیری‌مان اثر می‌دادیم با این حال می‌دانیم که بسیاری از بازی‌ها تماماً مشاهده‌پذیر نیستند. به همین جهت لازم است تا از الگوریتم‌های دیگری استفاده کنیم. در بخش قبلی با الگوریتم Minimax ، هرس کردن آن و قطع کردن آن به کمک تابع ارزیابی آشنا شدید؛ در این بخش می‌خواهیم یک عامل Minimax را ابتدائا پیاده‌سازی کنیم و سپس آن را هرس کنیم. لازم است توجه داشته باشیم که به دلیل زیاد بودن حجم ما حتما درخت را از جایی قطع می‌کنیم و مثلا در عمق دو از تابع ارزیابی استفاده می‌کنیم. از سوی دیگر به جهت وجود چند روح درخت ما کمی متفاوت از آن چیزی‌ست که در بخش قبلی دیدیم و به ازای هر روی یک لایهٔ Min وجود دارد. برای انجام این بخش لازم است که تابع ارزیابی شما باید وضعیت‌ها را ارزیابی کند نه actionها را.\n",
    "        <br>         \n",
    "        <br>\n",
    "        با این توضیحات بیایید تا فرآیند پیاده‌سازی این عامل را بررسی کنیم. برای این‌که فعلا درگیر تابع ارزیابی نشویم تصور کنید در هر لحظه از زمان این مقدار برابر مقدار امتیاز پک‌من است. خب حالا یک بار فرآیند درخت minimax را در ذهن خود بیاورید. ما در هر طبقه رئوسی داشتیم. با توجه به نوع طبقه که max هست یا min، رئوس مقادیر بیشینه یا کمینه فرزندان‌شان را در درخت می‌پذیرفتند و اگر در درخت به قدر کافی پایین رفته بودیم مقدار رأس مورد نظر برابر مقدار تابع ارزیابی در آن رأس (وضعیت) بود. در نهایت مقدار  رأس ریشه به عنوان ماکسیمم انتخاب می‌شود. با توجه به این توضیحات سعی کنید که این عامل را پیاده‌سازی کنید.\n",
    "        <br>\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MinimaxAgentUtil.py\n",
    "from util import manhattanDistance\n",
    "from game import Directions\n",
    "def minimaxAgentAction(agent, gameState):\n",
    "    \"\"\"\n",
    "      Returns the minimax action from the current gameState using self.depth\n",
    "      and self.evaluationFunction.\n",
    "\n",
    "      Here are some method calls that might be useful when implementing minimax.\n",
    "\n",
    "      gameState.getLegalActions(agentIndex):\n",
    "        Returns a list of legal actions for an agent\n",
    "        agentIndex=0 means Pacman, ghosts are >= 1\n",
    "\n",
    "      gameState.generateSuccessor(agentIndex, action):\n",
    "        Returns the successor game state after an agent takes an action\n",
    "\n",
    "      gameState.getNumAgents():\n",
    "        Returns the total number of agents in the game\n",
    "    \"\"\"\n",
    "    #your code here\n",
    "return Directions.STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Pacman\n",
    "%run pacman.py -p MinimaxAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "        پیشنهاد می‌کنیم بعد از پیاده‌سازی کد خودتان به کد پایین هم به عنوان یک نمونه نگاهی بیندازید.\n",
    "        <br>\n",
    "       حالا می‌خواهیم Minimax را پیاده سازی کنیم. فرض می‌کنیم ارواح بین حرکت‌های ممکن یکی راه به صورت شانسی به احتمال برابر انجام می‌دهند.\n",
    "         برای ارواح Reflex Agent به صورت پیش‌فرض RandomGhost است.\n",
    "        <br>\n",
    "        <br>  \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MinimaxAgentUtil.py\n",
    "from util import manhattanDistance\n",
    "from game import Directions\n",
    "def minimaxAgentAction(agent, gameState):\n",
    "    maxValue = float(\"-inf\")\n",
    "    maxAction = Directions.STOP\n",
    "    for action in gameState.getLegalActions(0):\n",
    "        nextState = gameState.generateSuccessor(0, action)\n",
    "        nextValue = getValue(agent, nextState, 0, 1)\n",
    "        if nextValue > maxValue:\n",
    "            maxValue = nextValue\n",
    "            maxAction = action\n",
    "    return maxAction\n",
    "\n",
    "def getValue(agent, gameState, currentDepth, agentIndex):\n",
    "    if currentDepth == agent.depth or gameState.isWin() or gameState.isLose():\n",
    "        return agent.evaluationFunction(gameState)\n",
    "    elif agentIndex == 0:\n",
    "        return maxValue(agent, gameState,currentDepth)\n",
    "    else:\n",
    "        return minValue(agent, gameState,currentDepth,agentIndex)\n",
    "\n",
    "def maxValue(agent, gameState, currentDepth):\n",
    "    maxValue = float(\"-inf\")\n",
    "    for action in gameState.getLegalActions(0):\n",
    "        maxValue = max(maxValue, getValue(agent, gameState.generateSuccessor(0, action), currentDepth, 1))\n",
    "    return maxValue\n",
    "\n",
    "def minValue(agent, gameState, currentDepth, agentIndex):\n",
    "    minValue = float(\"inf\")\n",
    "    for action in gameState.getLegalActions(agentIndex):\n",
    "        if agentIndex == gameState.getNumAgents()-1:\n",
    "            minValue = min(minValue, getValue(agent, gameState.generateSuccessor(agentIndex, action), currentDepth+1, 0))\n",
    "        else:\n",
    "            minValue = min(minValue, getValue(agent, gameState.generateSuccessor(agentIndex, action), currentDepth, agentIndex+1))\n",
    "    return minValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Pacman\n",
    "%run pacman.py -p MinimaxAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pacman.py -p MinimaxAgent -n 10 -f -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "        با اجرای کد بالا می‌بینید که تنها ۲ بازی را می‌برد.\n",
    "        <br>\n",
    "        در کد بالا برای اینکه امتیاز یک وضعیت را به دست بیاوریم اگر به عمق agent.depth رسیده بودیم. از تابع امتیاز‌دهی agent استفاده می‌کند که فعلا چیز خاصی برایش پیاده سازی نشده است. (مقدار پیشفرض agent.depth برابر ۲ است.)\n",
    "        <br>\n",
    "        اگر به عمق agent.depth نرسیده بودیم ابتدا پکمن بین حالت‌های مخلتف هرکدام را انتخاب می‌کند و وضعیت‌های جدید بازی را می‌سازد و بین امتیاز وضعیت‌های ممکن بیشترین را انتخاب می‌کنند و به صورت بازگشتی امتیاز را برای وضعیت‌ها حساب می‌کنند. و برای ارواح هم کدام حرکت خود را انجام می‌دهند و تعدادی وضعیت جدید بازی می‌سازند و بین آن‌ها وضعیت با امتیاز کمینه را انتخاب می‌کنند.\n",
    "        <br>\n",
    "        در واقع اگر در طبقه max بودیم یعنی پکمن باید حرکت کند و اگر در طبقه min بودیم ارواح باید حرکت کنند. نکته ای که وجود دارد این است که ارواح و پکمن همزمان حرکت می‌کنند ولی ما فرض کردیم که ابتدا پکمن یک حرکت می‌کنند و سپس ارواح هر کدام یک حرکت انجام می‌دهند. \n",
    "        <br>\n",
    "        اگر یک بار کد را خط به خط اجرا کنید دقیق‌تر متوجه می‌شوید که این کد چگونه دارد کار می‌کند.\n",
    "        <br>\n",
    "        اگر ارواح را با شماره‌های 1 تا k شماره گذاری کنیم. k برابر gameState.getNumAgents()-1 است چون یکی از agent ها خود پکمن است.\n",
    "        <br>\n",
    "        برای اینکه حرکات مجاز پکمن را به دست بیاورید از تابع \n",
    "        getLegalActions(0) می‌توانید استفاده کنید. و برای اینکه حرکت‌های مجاز روح i ام را به دست بیاورید از تابع\n",
    "        getLegalActions(i)\n",
    "        می‌توانید استفاده کنید.\n",
    "        <br>\n",
    "        gameState.generateSuccessor(i, action)\n",
    "        وضعیت جدید بازی را درست می‌کند اگر روح i ام عملگر action را انجام بدهد. (اگر i برابر ۰ باشد پکمن این عملگر را انجام می‌دهد.)\n",
    "        <br>\n",
    "        <br> \n",
    "        حال می‌خواهیم تابع امتیاز دهی را بهتر کنیم.\n",
    "        کد زیر را ببینید.\n",
    "        <br>\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MinimaxAgentEvalUtil.py\n",
    "import util\n",
    "def minimaxAgentEval(gameState):\n",
    "    pos = gameState.getPacmanPosition()\n",
    "    food = gameState.getFood()\n",
    "    ghostStates = gameState.getGhostStates()\n",
    "\n",
    "    if gameState.isWin():\n",
    "        return float(\"inf\")\n",
    "\n",
    "    for ghostState in ghostStates:\n",
    "        if util.manhattanDistance(ghostState.getPosition(), pos) < 2 and ghostState.scaredTimer < 3:\n",
    "            return float(\"-inf\")\n",
    "\n",
    "    foodDist = []\n",
    "    for food in list(food.asList()):\n",
    "        foodDist.append(util.manhattanDistance(food, pos))\n",
    "\n",
    "    return gameState.getScore() - 5 * min(foodDist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MinimaxAgentUtil.py\n",
    "from util import manhattanDistance\n",
    "from game import Directions\n",
    "from MinimaxAgentEvalUtil import minimaxAgentEval\n",
    "def minimaxAgentAction(agent, gameState):\n",
    "    maxValue = float(\"-inf\")\n",
    "    maxAction = Directions.STOP\n",
    "    for action in gameState.getLegalActions(0):\n",
    "        nextState = gameState.generateSuccessor(0, action)\n",
    "        nextValue = getValue(agent, nextState, 0, 1)\n",
    "        if nextValue > maxValue:\n",
    "            maxValue = nextValue\n",
    "            maxAction = action\n",
    "    return maxAction\n",
    "\n",
    "def getValue(agent, gameState, currentDepth, agentIndex):\n",
    "    if currentDepth == agent.depth or gameState.isWin() or gameState.isLose():\n",
    "        \n",
    "        ########## this line is new ###########\n",
    "        return minimaxAgentEval(gameState)\n",
    "        #######################################\n",
    "        \n",
    "    elif agentIndex == 0:\n",
    "        return maxValue(agent, gameState,currentDepth)\n",
    "    else:\n",
    "        return minValue(agent, gameState,currentDepth,agentIndex)\n",
    "\n",
    "def maxValue(agent, gameState, currentDepth):\n",
    "    maxValue = float(\"-inf\")\n",
    "    for action in gameState.getLegalActions(0):\n",
    "        maxValue = max(maxValue, getValue(agent, gameState.generateSuccessor(0, action), currentDepth, 1))\n",
    "    return maxValue\n",
    "\n",
    "def minValue(agent, gameState, currentDepth, agentIndex):\n",
    "    minValue = float(\"inf\")\n",
    "    for action in gameState.getLegalActions(agentIndex):\n",
    "        if agentIndex == gameState.getNumAgents()-1:\n",
    "            minValue = min(minValue, getValue(agent, gameState.generateSuccessor(agentIndex, action), currentDepth+1, 0))\n",
    "        else:\n",
    "            minValue = min(minValue, getValue(agent, gameState.generateSuccessor(agentIndex, action), currentDepth, agentIndex+1))\n",
    "    return minValue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Pacman\n",
    "%run pacman.py -p MinimaxAgent  -n 10 -f -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "        با اجرای کد بالا می‌بینید  وضع بدتر شد!\n",
    "         چون تابع امتیاز دهی ما خوب نبود.\n",
    "        می‌خواهیم تغییراتی در تابع minimaxAgentEval بدهیم که بهتر شود.\n",
    "        <br>\n",
    "        <br>\n",
    "        <br>  \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MinimaxAgentEvalUtil.py\n",
    "import util\n",
    "def minimaxAgentEval(gameState):\n",
    "    pos = gameState.getPacmanPosition()\n",
    "    food = gameState.getFood()\n",
    "    ghostStates = gameState.getGhostStates()\n",
    "    # scaredTimes = [ghostState.scaredTimer for ghostState in ghostStates]\n",
    "\n",
    "    if gameState.isWin():\n",
    "        return float(\"inf\")\n",
    "\n",
    "    for ghostState in ghostStates:\n",
    "        if util.manhattanDistance(ghostState.getPosition(), pos) < 2 and ghostState.scaredTimer < 3:\n",
    "            return float(\"-inf\")\n",
    "\n",
    "    foodDist = []\n",
    "    for food in list(food.asList()):\n",
    "        foodDist.append(util.manhattanDistance(food, pos))\n",
    "\n",
    "    \n",
    "    return gameState.getScore() - 50 * min(foodDist) + 5*gameState.getNumFood()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Pacman\n",
    "%run pacman.py -p MinimaxAgent  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "        مشکلی که تابع قبلی داشت این بود که باعث می‌شد پکمن گاهی گیر کند و حرکت خاصی انجام ندهد.\n",
    "        برای اینکار ما خروجی را با ۵ برابر تعداد غذا‌های استیت جمع\n",
    "        کردیم تا اگر می‌شد به استیتی با غذای کمتر رفت گیر نکند.\n",
    "        چون پکمن نزدیک غذا می‌شد و بعد گیر می‌کرد.\n",
    "        اما با اجرای کد بالا می‌بیند فایده ای نداشت تلاش‌هامان!\n",
    "        مشکل این است که ما برای فاصله از فاصله منهتنی استفاده کردیم ولی این را در نظر نگرفتیم که پکمن از دیوار نمی تواند رد شود.\n",
    "        مشکل دیگر این است که وقتی غذا را می‌خورد فاصله نزدیک‌ترین غذا زیاد می‌شود. چون نزدیک ترین غذا را خورده بود.\n",
    "        پس باید تاثیر غذا خوردن را زیادتر کنیم.\n",
    "        کد زیر این مشکل را رفع می‌کند.\n",
    "        <br>\n",
    "        <br>\n",
    "        <br>  \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile MinimaxAgentEvalUtil.py\n",
    "import util\n",
    "from bfs import find_closest_dot\n",
    "def minimaxAgentEval(gameState):\n",
    "    pos = gameState.getPacmanPosition()\n",
    "    ghostStates = gameState.getGhostStates()\n",
    "    # scaredTimes = [ghostState.scaredTimer for ghostState in ghostStates]\n",
    "\n",
    "    if gameState.isWin():\n",
    "        return float(\"inf\")\n",
    "\n",
    "    for ghostState in ghostStates:\n",
    "        if util.manhattanDistance(ghostState.getPosition(), pos) < 2 and ghostState.scaredTimer < 3:\n",
    "            return float(\"-inf\")\n",
    "\n",
    "    foodDist = find_closest_dot(gameState)\n",
    "    return gameState.getScore() - 1000 * foodDist - 5000000*gameState.getNumFood()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Pacman\n",
    "\n",
    "%run pacman.py -p MinimaxAgent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pacman.py -p MinimaxAgent  -n 10 -f -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "        اگر تعداد غذاها در یک استیت بازی کمتر شود امتیاز آن استیت نسبت به بقیه استیت‌ها به شدت زیاد می‌شود.\n",
    "        با اجرای کد بالا می‌بینید که از ۱۰ بازی ۹ بازی را می‌برد.\n",
    "        <br>\n",
    "        اگر بازی دوم را ببینید متوجه می‌شوید پکمن در موقعیتی قرار می‌گیرد که در دوطرفش روح وجود دارد و نمی‌تواند در برود.\n",
    "        <br>\n",
    "        سعی کنید این مشکل را حل کنید تا به برد ۱۰ از ۱۰ برسید\n",
    "        <br>  \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "    بعد از پیاده‌سازی عامل Minimax به سراغ حرص آن می‌رویم. در این لحظه پیشنهاد می‌کنم از خواندن ادامهٔ این متن دست بکشید و سعی کنید اندکی به هدف و ایدهٔ این الگوریتم فکر کنید و برای خود آن را توضیح دهید سپس به خواندن ادامه دهید. <br><br>در این الگوریتم فرض بر آن است که اگر در حال محاسبهٔ مثلا یک رأس کمینه هستیم و یک عدد کوچک مثل بتا داریم دیگر لازم نیست شاخه‌هایی که مقدار تابع ارزیابی‌شان بیش از بتا است را بررسی کنیم زیرا می‌دانیم پاسخ این رأس کمینه نخواهند بود و همین قاعده به طور برعکس برای رئوس بیشینه و آلفا نیز برقرار است. همچنین اگر به عددی کوچک‌تر (بزرگ‌تر) از بتا (آلفا) برخوردیم مقدار بتا (آلفا) را آپدیت می‌کنیم. به شکل زیر توجه کنید \n",
    "    <img src=\"image/alpha-beta.png\" align=\"middle\" height=\"500\" width=\"500\"/>\n",
    "        حال به سراغ پیاده‌سازی همین ایدهٔ ساده با تغییر دادن عاملی که برای الگوریتم minimax نوشته بودیم می‌رویم. به پیاده‌سازی ساده زیر دقت کنید و آن را امتحان کنید. \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile AlphaBetaAgentUtil.py\n",
    "from util import manhattanDistance\n",
    "from game import Directions\n",
    "from MinimaxAgentEvalUtil import minimaxAgentEval\n",
    "\n",
    "def alphaBetaAgentAction(agent, gameState):\n",
    "    maxValue = float(\"-inf\")\n",
    "    alpha = float(\"-inf\")\n",
    "    beta = float(\"inf\")\n",
    "    maxAction = Directions.STOP\n",
    "    for action in gameState.getLegalActions(0):\n",
    "        nextState = gameState.generateSuccessor(0, action)\n",
    "        nextValue = getValue(agent, nextState, 0, 1, alpha, beta)\n",
    "        if nextValue > maxValue:\n",
    "            maxValue = nextValue\n",
    "            maxAction = action\n",
    "        alpha = max(alpha, maxValue)\n",
    "    return maxAction\n",
    "\n",
    "def getValue(agent, gameState, currentDepth, agentIndex, alpha, beta):\n",
    "    if currentDepth == agent.depth or gameState.isWin() or gameState.isLose():\n",
    "        return minimaxAgentEval(gameState)\n",
    "    elif agentIndex == 0:\n",
    "        return maxValue(agent, gameState,currentDepth,alpha,beta)\n",
    "    else:\n",
    "        return minValue(agent, gameState,currentDepth,agentIndex,alpha,beta)\n",
    "\n",
    "def maxValue(agent, gameState, currentDepth, alpha, beta):\n",
    "    maxValue = float(\"-inf\")\n",
    "    for action in gameState.getLegalActions(0):\n",
    "        maxValue = max(maxValue, getValue(agent, gameState.generateSuccessor(0, action), currentDepth, 1, alpha, beta))\n",
    "        if maxValue > beta:\n",
    "            return maxValue\n",
    "        alpha = max(alpha, maxValue)\n",
    "    return maxValue\n",
    "\n",
    "def minValue(agent, gameState, currentDepth, agentIndex, alpha, beta):\n",
    "    minValue = float(\"inf\")\n",
    "    for action in gameState.getLegalActions(agentIndex):\n",
    "        if agentIndex == gameState.getNumAgents()-1:\n",
    "            minValue = min(minValue, getValue(agent, gameState.generateSuccessor(agentIndex, action), currentDepth+1, 0, alpha, beta))\n",
    "        else:\n",
    "            minValue = min(minValue, getValue(agent, gameState.generateSuccessor(agentIndex, action), currentDepth, agentIndex+1, alpha, beta))\n",
    "        if minValue < alpha:\n",
    "            return minValue\n",
    "        beta = min(beta, minValue)\n",
    "    return minValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Pacman\n",
    "%run pacman.py -p AlphaBetaAgent -f "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "       عملکرد دو عامل minimax و alpha beta را چگونه می‌بینید؟ آیا دچار حلقه‌های بی‌پایان در تصمیم‌گیری پک‌من می‌شوند؟ \n",
    "       حرص آلفا بتا در سرعت و عملکرد تاثیر دارد در کد minimax قبلی نمی‌توانستیم عمق تصمیم گیری را بیشتر از ۲ بکنیم چون برنامه کند می‌شد اما در اینجا می‌توانیم عمق را ۳ بکنیم.\n",
    "        هرچند بازم کند است اما خیلی بهتر از حالت قبلی می‌شود.\n",
    "        به صورت پیشفرض عمق ۲ است که در agent.depth ذخیره شده است.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile AlphaBetaAgentUtil.py\n",
    "from util import manhattanDistance\n",
    "from game import Directions\n",
    "from MinimaxAgentEvalUtil import minimaxAgentEval\n",
    "\n",
    "def alphaBetaAgentAction(agent, gameState):\n",
    "    ################################\n",
    "    ## we added this line to code\n",
    "    agent.depth = 3\n",
    "    ################################\n",
    "    maxValue = float(\"-inf\")\n",
    "    alpha = float(\"-inf\")\n",
    "    beta = float(\"inf\")\n",
    "    maxAction = Directions.STOP\n",
    "    for action in gameState.getLegalActions(0):\n",
    "        nextState = gameState.generateSuccessor(0, action)\n",
    "        nextValue = getValue(agent, nextState, 0, 1, alpha, beta)\n",
    "        if nextValue > maxValue:\n",
    "            maxValue = nextValue\n",
    "            maxAction = action\n",
    "        alpha = max(alpha, maxValue)\n",
    "    return maxAction\n",
    "\n",
    "def getValue(agent, gameState, currentDepth, agentIndex, alpha, beta):\n",
    "    if currentDepth == agent.depth or gameState.isWin() or gameState.isLose():\n",
    "        return minimaxAgentEval(gameState)\n",
    "    elif agentIndex == 0:\n",
    "        return maxValue(agent, gameState,currentDepth,alpha,beta)\n",
    "    else:\n",
    "        return minValue(agent, gameState,currentDepth,agentIndex,alpha,beta)\n",
    "\n",
    "def maxValue(agent, gameState, currentDepth, alpha, beta):\n",
    "    maxValue = float(\"-inf\")\n",
    "    for action in gameState.getLegalActions(0):\n",
    "        maxValue = max(maxValue, getValue(agent, gameState.generateSuccessor(0, action), currentDepth, 1, alpha, beta))\n",
    "        if maxValue > beta:\n",
    "            return maxValue\n",
    "        alpha = max(alpha, maxValue)\n",
    "    return maxValue\n",
    "\n",
    "def minValue(agent, gameState, currentDepth, agentIndex, alpha, beta):\n",
    "    minValue = float(\"inf\")\n",
    "    for action in gameState.getLegalActions(agentIndex):\n",
    "        if agentIndex == gameState.getNumAgents()-1:\n",
    "            minValue = min(minValue, getValue(agent, gameState.generateSuccessor(agentIndex, action), currentDepth+1, 0, alpha, beta))\n",
    "        else:\n",
    "            minValue = min(minValue, getValue(agent, gameState.generateSuccessor(agentIndex, action), currentDepth, agentIndex+1, alpha, beta))\n",
    "        if minValue < alpha:\n",
    "            return minValue\n",
    "        beta = min(beta, minValue)\n",
    "    return minValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Pacman\n",
    "%run pacman.py -p AlphaBetaAgent -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "    حال می‌خواهیم عملکرد کدی که زدیم را در مقابل یک عامل بازتابی دیگر برای ارواح بررسی کنیم.\n",
    "        عامل بازتابی DirectionalGhost اینگونه است که ارواح به دنبال پکمن می‌روند.\n",
    "        به خاطر سریعتر شدن کد عمق را به ۲ بر می‌گردانیم.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile AlphaBetaAgentUtil.py\n",
    "from util import manhattanDistance\n",
    "from game import Directions\n",
    "from MinimaxAgentEvalUtil import minimaxAgentEval\n",
    "\n",
    "def alphaBetaAgentAction(agent, gameState):\n",
    "\n",
    "    maxValue = float(\"-inf\")\n",
    "    alpha = float(\"-inf\")\n",
    "    beta = float(\"inf\")\n",
    "    maxAction = Directions.STOP\n",
    "    for action in gameState.getLegalActions(0):\n",
    "        nextState = gameState.generateSuccessor(0, action)\n",
    "        nextValue = getValue(agent, nextState, 0, 1, alpha, beta)\n",
    "        if nextValue > maxValue:\n",
    "            maxValue = nextValue\n",
    "            maxAction = action\n",
    "        alpha = max(alpha, maxValue)\n",
    "    return maxAction\n",
    "\n",
    "def getValue(agent, gameState, currentDepth, agentIndex, alpha, beta):\n",
    "    if currentDepth == agent.depth or gameState.isWin() or gameState.isLose():\n",
    "        return minimaxAgentEval(gameState)\n",
    "    elif agentIndex == 0:\n",
    "        return maxValue(agent, gameState,currentDepth,alpha,beta)\n",
    "    else:\n",
    "        return minValue(agent, gameState,currentDepth,agentIndex,alpha,beta)\n",
    "\n",
    "def maxValue(agent, gameState, currentDepth, alpha, beta):\n",
    "    maxValue = float(\"-inf\")\n",
    "    for action in gameState.getLegalActions(0):\n",
    "        maxValue = max(maxValue, getValue(agent, gameState.generateSuccessor(0, action), currentDepth, 1, alpha, beta))\n",
    "        if maxValue > beta:\n",
    "            return maxValue\n",
    "        alpha = max(alpha, maxValue)\n",
    "    return maxValue\n",
    "\n",
    "def minValue(agent, gameState, currentDepth, agentIndex, alpha, beta):\n",
    "    minValue = float(\"inf\")\n",
    "    for action in gameState.getLegalActions(agentIndex):\n",
    "        if agentIndex == gameState.getNumAgents()-1:\n",
    "            minValue = min(minValue, getValue(agent, gameState.generateSuccessor(agentIndex, action), currentDepth+1, 0, alpha, beta))\n",
    "        else:\n",
    "            minValue = min(minValue, getValue(agent, gameState.generateSuccessor(agentIndex, action), currentDepth, agentIndex+1, alpha, beta))\n",
    "        if minValue < alpha:\n",
    "            return minValue\n",
    "        beta = min(beta, minValue)\n",
    "    return minValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Pacman\n",
    "%run pacman.py -p AlphaBetaAgent -f -n 10 -q -g DirectionalGhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "    <font face=\"XB Zar\" size=5>\n",
    "    همانطور که می‌بینید پکمن ما ۳ بار می‌بازد.\n",
    "    برای تمرین بیشتر پیشنهاد می‌شود که هم برای پکمن و هم برای ارواح سعی کنید عامل بازتابی بنویسید.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reflex\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "        <font color=#FF7500 size=6>\n",
    "منابع\n",
    "        </font>\n",
    "        <br>\n",
    "        <a href=\"https://en.wikipedia.org/wiki/Intelligent_agent\"> معرفی اجمالی عامل‌های هوشمند </a>\n",
    "        <br>\n",
    "        <a href=\"http://ai.berkeley.edu/multiagent.html\">محتوای آموزشی پک‌من برای درس هوش مصنوعی دانشگاه برکلی</a>\n",
    "    </font>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
