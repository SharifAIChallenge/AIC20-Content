{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<div align=center>\n",
    "\t\t\t<p></p>\n",
    "\t\t\t<br />\n",
    "            <img src=\"images/RL.jpg\" style=\"border-radius: 5px\">\n",
    "            <br />\n",
    "\t\t</div>\n",
    "\t\t<style type=\"text/css\" scoped>\n",
    "        p{\n",
    "        border: 1px solid #a2a9b1;background-color: #f8f9fa;display: inline-block;\n",
    "        };\n",
    "        </style>\n",
    "\t\t<div>\n",
    "\t\t\t<h3>فهرست مطالب</h3>\n",
    "\t\t\t<ul style=\"margin-right: 0;\">\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#start\">\n",
    "                        شروعی مختصر و مفید\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#what\">\n",
    "                        یادگیری تقویتی چیست و چرا\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#qlearning\">\n",
    "                        ایده‌ی بدیهی اول، گشت تصادفی\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#try\">\n",
    "                        تلاشی برای دقیق‌تر کردن شرط‌های لازم برای این الگوریتم\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#simple\">\n",
    "                       یک الگوریتم ساده: گشت تصادفی به همراه بروزرسانی Q مقدارها\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#feature\">\n",
    "                    عه! پس کلا مساله حل شد و تمام؟\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#packman\">\n",
    "                    جدی جدی وقت یادگیری یک بازیه!\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "        </div>\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"start\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "شروعی مختصر و مفید\n",
    "        </font>\n",
    "        <p></p>\n",
    "        با سلام! وقت کم است و اعمال بسیار! بیایید هر چه سریعتر با اجرای دستور زیر وارد پوشه‌ی پروژه‌ی سوم درس هوش مصنوعی شوید تا آموزش یادگیری تقویتی را شروع کنیم.\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Project3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"start\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "        <p></p>\n",
    "      با اجرای تکه‌ی کد زیر می‌توانید کد‌ها را در همین یادداشت تغییر دهید و نتیجه را در عمل ببینید.\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"what\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "یادگیری تقویتی چیست؟\n",
    "        </font>\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "        در یادداشت قبلی، با\n",
    "         MDP\n",
    "        ها آشنا شدیم.\n",
    "        هدف ما در این یادداشت این است که اگر ما محیط اطراف را یک  MDP \n",
    "        در نظر بگیریم، حال می‌خواهیم الگوریتمی بنویسیم که به صورت بهینه در این محیط حرکت کند.\n",
    "        در ابتدا دقت کنید که اگر خصوصیات MDP\n",
    "        را به صورت کامل داشته باشیم می‌توانیم مسیر بهینه را مشخص کنیم.\n",
    "        اما چطور باید خصوصیات MDP را  \n",
    "        به دست بیاوریم و آن‌ها را محاسبه کنیم؟\n",
    "        الگوریتم ما باید با محیط تعامل کند  و خصوصیات محیط را یاد بگیرد\n",
    "        روش‌های زیادی برای یاد گرفتن MDP و به دست آوردن\n",
    "        استراتژی بهینه از روی داده‌های به دست آمده از این تعامل با محیط وجود دارند. \n",
    "        یکی از معروف‌ترین این الگوریتم‌ها به\n",
    "       Q-learning\n",
    "       مشهور است.\n",
    "       این الگوریتم به صورت مستقیم به محاسبه‌ی \n",
    "        Q-مقدار‌ها می‌پردازد.\n",
    "       در واقع این الگوریتم محیط\n",
    "        gridworld\n",
    "        تحویل می‌گیرد و سپس از روی آن \n",
    "        Q-مقدار‌ها \n",
    "        را یاد می‌گیرد.\n",
    "         اگر از یادداشت قبل به خاطر داشته‌باشید، می‌دانید که با داشتن\n",
    "        Q-مقدار‌ها\n",
    "        محاسبه‌ي بهترین کنش یا عمل در هر لحظه بسیار ساده است. در ادامه به الگوریتمی برای چگونه یاد گرفتن این مقادیر می‌پردازیم.\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"what\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "بازی اندکی با محیط\n",
    "        </font>\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "     بهتر است قبل از هر چیز، اندکی با محیط gridworld آشنا شوید.\n",
    "        به این منظور بهترین راه این است که آن را اجرا کنید و کمی هم در آن بازی کنید. برای اجرای محسط بازی می‌توانید از دستور زیر استفاده کنید. دقت داشته باشید که پارامتر‌های ورودی را می‌توانید به دلخواه تغییر دهید.\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py --noise 0.2 --livingReward 0 -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"qlearning\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "یاد گرفتن Q-مقدار‌ها \n",
    "        </font>\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "        یک الگوریتم بسیار ساده این است که از یکی از وضعیت‌ها شروع کرده و سپس به صورت تصادفی یکی از اعمالی را که در اختیار ماست انتخاب کنیم و آن را انجام دهیم. \n",
    "        بگذارید در ابتدا یک اجرا از این الگوریتم را با همدیگر ببینیم:\n",
    "        در اجرایی که در زیر مشاهده می‌کنید، الگوریتم هوشمند ما به صورت رندوم ۲۰ بار در \n",
    "        gridworld\n",
    "        بازی می‌کند.\n",
    "        البته قابل توجه است که متاسفانه تصادفی بازی کردن، چیزی نیست که به آن هوشمندی بگوییم اما در ادامه خواهید دید که همین  بازی تصادفی هم در نهایت منجر به درک دقیقی از محیط می‌شود.\n",
    "         چگونه از این حرکات تصادفی که در زیر می‌بینید، عاملی هوشمند سر در خواهد آورد؟\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -k 20 -a random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "سوالی که در این‌جا مطرح است این است که ما از چنین بازی تصادفی‌ای چه چیزی را می‌توانیم یاد بگیریم؟\n",
    " چه چیزی را می‌خواهیم بدانیم؟ ما در هر وضعیت می‌خواهیم بدانیم که اگر یک \n",
    "کنش از بین کنش‌هایی که در اختیار داریم را انتخاب کنیم و پس از دریافت جایزه‌ی آن کنش دیگر بهینه عمل کنیم در انتها به چه میزان\n",
    "جایزه دست خواهیم یافت. البته دقت دارید که در اینجا، منظور از جایزه، امید ریاضی جایزه است.\n",
    "در واقع توصیف دقیق‌تر این است که می‌خواهیم بدانیم در صورتی که یک کنش را انجام دهیم و از آن پس بهینه عمل کنیم،\n",
    "امید ریاضی جایزه‌ای که به آن دست پیدا خواهیم کرد چه مقداری است.\n",
    "اما چه باید بکنیم؟\n",
    "در ابتدا از روش‌های بسیار ابتدایی شروع می‌کنیم تا روش‌های بهتر و بهتری را خلق کنیم.\n",
    "<p></p>\n",
    "\n",
    "<font color=#FF7500 size=6>\n",
    "ایده‌ی اول: گشت تصادفی \n",
    " </font>\n",
    "ما در اینجا هیچ شهودی نسبت به احتمالات یا جایزه‌ها نداریم. بنابراین تنها راه چاره‌ی ما گشت زدن در محیط است.\n",
    "یک راه بسیار ساده این است که بارها یک مسیر را طی‌کنیم و مسیر‌های مختلف را امتحان کنیم تا آرام آرام به شهودی نسبت به محیط دست پیدا کنیم.\n",
    "فرض کنید که اینگونه شروع کنیم: در یک وضعیت قرار بگیریم و به صورت تصادفی به سمت بالا برویم.\n",
    "و همینطور تصادفی حرکت کنیم  تا وقتی که \n",
    "بازی تمام شود یا در واقع یک اپیزود از بازی تمام ‌شود. فرض کنید که در این اجرای از بازی ما به جایزه‌ی \n",
    "100\n",
    "رسیده باشیم.\n",
    "حالا شهود شما در مورد وضعیت اول چیست؟\n",
    "اولین چیزی که به ذهن می‌رسد این است که با خود بگوییم اگر به سمت بالا برویم، می‌توانیم 100\n",
    "امتیاز جمع کنیم. این شهود لزوما درست نیست و اشکال‌های زیادی دارد:\n",
    "\n",
    "<ul>\n",
    "<li>\n",
    "ما بهینه عمل نکرده‌ایم. ما صرفا در یک اجرا به سمت بالا، به امتیاز 100 دست یافته‌ایم. این به هیچ وجه دلیلی بر این نیست که در \n",
    "حرکت بهینه به سمت بالا ما نتوانیم بیشتر از صد امتیاز بگیریم.\n",
    "در واقع ما پس از حرکت به سمت بالا تصادفی عمل کرده‌ایم و برای همین می‌توان گفت که شاید حرکت هوشمندانه‌تری موجود باشد که ما رابه \n",
    "جایزه‌ای بیش از 100 برساند\n",
    "(دقت داشته باشید که منظور امید ریاضی جایزه است).\n",
    "</li>\n",
    "<li>\n",
    "شاید اتفاقا در همین حرکت تصادفی هم بسیار خوش‌شانس بوده‌ایم که امتیاز 100\n",
    "را بدست آورده‌ایم.\n",
    "شاید در عمل در این محیط اگر در اول به سمت بالا حرکت کنیم و سپس بهینه حرکت کنیم،\n",
    "نهایتا به امیدریاضی \n",
    "80 بتوانیم برسیم.\n",
    "چگونه می‌خواهیم مطمئن شویم که چنین اتفاقی رخ نداده است؟\n",
    "</li>\n",
    "</ul>\n",
    "در این باره الگوریتمی وجود دارد که به ما این اطمینان را می‌دهد که تمام\n",
    "Q\n",
    "مقدار‌ها را به طور دقیق پیدا کند.\n",
    "البته منظور از دقیق این است که این الگوریتم ما به ما اطمینان می‌دهد که در یک محیط مارکوف،\n",
    "در صورتی که تا زمان بی‌نهایت اجرا شود مقادیری را به عنوان \n",
    "Q\n",
    "مقدار‌ها به ما گزارش می‌دهد که به\n",
    "Q\n",
    "مقدارهای دقیق همگرا هستند.\n",
    "به نظر شما چنین الگوریتمی، چگونه عمل می‌کند؟\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"try\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "تلاشی برای دقیق‌تر کردن شرط‌های لازم برای این الگوریتم\n",
    " </font>\n",
    "اولین ایده این است که ما باید در هر وضعیت، حتما تمام کنش‌ها را حداقل یکبار امتحان کنیم زیرا ممکن است آن کنش جایزه‌ای بسیار بزرگ داشته باشد و در واقع با اختلاف بسیار زیاد با بقیه‌ی جوایز موجود در \n",
    "    آن محیط مارکوف از آن‌ها جلو باشد. تا وقتی آن را امتحان نکرده باشیم نمی‌توانیم نظر دقیقی در مورد آن محیط بدهیم.\n",
    "    پس اولین نتیجه‌گیری مهم این است که:\n",
    "    <p></p>\n",
    "    <b>\n",
    "        ما در هر وضعیت باید تمام کنش‌ها را حداقل یکبار بررسی کرده‌ باشیم.\n",
    "    </b>\n",
    "    <p></p>  \n",
    "    آیا شرط دیگری وجود دارد که حتما لازم باشد که چنین برنامه‌ای داشته باشد؟\n",
    "    برای مثال آیا می‌شود یک کنش را تعداد متناهی بار آزمایش کرد؟\n",
    "    فرض کنید که ما دو سکه‌ی شانس داریم و هر بار می‌توانیم یکی از آن‌ها را بیندازیم و اگر شیر بیاید، امتیازی را دریافت می‌کنیم و بعد از ده بار بازی تمام می‌شود.\n",
    "    چیزی که دراینجا مهم است این است که احتمال آمدن هیچ کدام از سمت‌ها را نمی‌دانیم.\n",
    "    در واقع این سکه‌ها سکه‌های عادلانه نیستند که هر سمت آن‌ها تنها به احتمال نیم بیاید. بلکه مثلا ممکن است سکه‌ی اول به احتمال\n",
    "    هشت دهم یا به احتمال سه دهم بیاید.\n",
    "    حال فرض کنید که جایزه‌ی هر بار انداختن هر سکه هم مشخص است و اگر شیر بیاید ما یک سکه‌ی طلا دریافت می‌کنیم.\n",
    "    به نظر شما اگر تنها به تعداد متناهی بار یکی از سکه‌ها را بیندازیم می‌توانیم مطمئن باشیم که \n",
    "    احتمال دقیق را محاسبه کرده‌ایم؟\n",
    "    جواب این سوال کمی به دانش ریاضی نیاز دارد اما به هر حال باید بدانیم که اگر بخواهیم احتمال شکل آمدن مثلا سکه‌ی اول را بدانیم باید آن را نامتناهی بار بیندازیم تا بتوانیم با تقسیم تعداد بارهایی که شیر آمده است بر تعداد بارهایی که خط آمده است احتمال شیر آمدن را حساب کنیم.\n",
    "    در واقع\n",
    "    $$\\frac{num(picturs)}{num(tries)}$$\n",
    "    وقتی به احتمال واقعی همگرا می‌شود که تعداد بارهای سکه انداختن به بی‌نهایت برسد.\n",
    "    پس در واقع می‌توان گفت:\n",
    "    <p></p>\n",
    "    <b>\n",
    "    الگوریتم ذکر شده هر چه باشد باید نامتناهی بار هر کنش را ارزیابی ‌کند.\n",
    "    </b>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"simple\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "یک الگوریتم ساده: گشت تصادفی به همراه بروزرسانی Q مقدار‌ها\n",
    " </font>\n",
    "فرض کنید که ما در یک وضعیت قرار می‌گیریم و سپس در هر مرحله به صورت تصادفی یک کنش را در محیط انجام می‌دهیم تا بازی تمام شود. و این  عمل را نامتناهی بار انجام می‌دهیم.\n",
    "    دقت کنید که این الگوریتم هر دو شرط ذکر شده در بالا را دارد. اما سوال اصلی این است: چگونه \n",
    "    Q \n",
    "    مقدار‌ها را بروزرسانی‌کنیم؟\n",
    "    در واقع چگونه با هر بار انتخاب یک کنش و مشاهده‌ی امتیاز‌ها\n",
    "    حدس خودمان از \n",
    "    Q \n",
    "    مقدار ها را بروزرسانی کنیم؟\n",
    "    یک ایده‌ی ابتدایی این است:\n",
    "   فرض کنید در وضعیت \n",
    "    $S$\n",
    "    هستیم وکنش\n",
    "    $a$\n",
    "    را از بین کنش‌های موجود انتخاب می‌کنیم.\n",
    "    سپس\n",
    "    $R(s,a,s')$\n",
    "    را دریافت می‌کنیم \n",
    "   . \n",
    "   به نظر شما چگونه می‌توانیم مقدار\n",
    "   $Q(s,a)$\n",
    "   را بروزرسانی کنیم تا شهود دقیق‌تری از مقدار آن داشته باشیم؟\n",
    "   <p></p>\n",
    "   یک ایده‌ی ساده این است که:\n",
    "   $$Q_{new}(s,a) = R(s,a,s') + \\gamma \\times Val(s')$$\n",
    "   یا به عبارت دیگر\n",
    "   $$Q_{new}(s,a) = R(s,a,s') + \\gamma \\times max_{a'\\in A(s)} Q(s', a')$$\n",
    "   در واقع کل حرفی که این عبارت ریاضی می‌زند این است که حالا که این مقدار از جایزه را با انتخاب این کنش دریافت کردم، اگر در \n",
    "   وضعیت \n",
    "   $s'$\n",
    "   هم بهترین انتخاب‌ها را داشته باشم\n",
    "   (\n",
    "   که در واقع همان قسمت عجیب‌غریب‌نمای\n",
    "   عبارت ریاضی بالاست که در واقع \n",
    "   بیان می‌کند که کدام کنش در وضعیت \n",
    "   $s'$\n",
    "   بهترین امیدریاضی را با توجه به حدس‌های ما تا الآن\n",
    "   ایفا می‌کند و سپس \n",
    "   مقدار آن امید ریاضی را برمی‌گرداند\n",
    "  )\n",
    "  چه امیدی می‌توانم با حدس‌های الآنم داشته باشم.\n",
    "  <p></p>\n",
    "  این عبارت، عبارت بسیار خوبی برای بروزرسانی است اما یک جای کار این عبارت می‌لنگد.\n",
    "  به نظر شما این نحو از بروزرسانی چه مشکلی می‌تواند داشته باشد؟\n",
    "  بیایید با همدیگر ببینیم:\n",
    "   \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qlearning_update.py\n",
    "def qlearning_update(self, state, action, nextState, reward):\n",
    "    estimatedQ = reward + self.discount * self.computeValueFromQValues(nextState)\n",
    "    self.qValues[(state,action)] = estimatedQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"random_learb\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "دقت کنید که پارامتر ورودی\n",
    "discount\n",
    "    پاس داده شده در واقع همان \n",
    "    gamma($\\gamma$)\n",
    "   است که ضریب کاهش پاداش است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -a q -k 30 -m --discount 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "مشکلات این نحوه از بروزرسانی شاید در ابتدا به نظر نرسند اما با کمی بررسی متوجه آن‌ها خواهید شد. خواهش ما این است که ابتدا سعی کنید خودتان مشکلات آن را بیابید و سپس به موارد زیر که به عقل ما رسیده است توجه کنید.\n",
    "<p></p>\n",
    "    بزرگترین مشکل این نحوه از بروزرسانی توجه به آخرین اتفاق در محیطی است که اتفاقات تصادفی هستند. به طور مثال در بازی\n",
    "   gridWorld\n",
    "   خانه‌ی سمت راست بالا امتیاز یک دارد. خانه‌ی سمت راست آن را در نظر بگیرید. حرکت به سمت راست در این خانه باید امیدریاضی بالایی داشته باشد.\n",
    "    زیرا در نزدیکی یک خاه‌ی یک امتیازی است و اگر حرکت به سمت راست موفقیت آمیز باشد ما در نزدیکی کسب یک امتیاز هستیم. اما حالا فرض کنید که موفقیت آمیز نباشد. بدین معنا که همانطور که می‌دانیم هر حرکت در\n",
    "    gridworld\n",
    "    با یک خطای اندک انجام می‌شود. ما به خانه‌ی پایین می‌رویم و باز هم شانس بازگشت به خانخه‌ی بالا را داریم.\n",
    "    اما نمونه‌ی زیر را نگاه کنید که در هنگام یادگیری اتفاق افتاده است.\n",
    "    در ابتدا وضعیت به صورت زیر بوده است:\n",
    "    <img src = \"images/direct_mistake.png\">    \n",
    "    <p></p>\n",
    "    حال ربات ما در وضعیت بالا تصمیم می‌گیرد به سمت راست برود اما دراینجا به سمت پایین می‌لغزد. همانطور که می‌بینید ناگهان ارزش حرکت به سمت راست در آن خانه برای او به شدت پایین می‌آید.\n",
    "    <img src = \"images/direct_mistake_2.png\">\n",
    "    <p></p>\n",
    " اما چنین اتفاقی منطقی نیست زیرا قبل از این اتفاق بارها همین ربات\n",
    "از طریق همین حرکت در همین خانه به خانه‌ی یک رفته بود. این کاهش شدید در ارزش منطقی نیست.\n",
    "    در واقع به صورت خلاصه مشکل این روش یادگیری\n",
    "    <b>\n",
    "        تصمیم گرفتن بر اساس آخرین اطلاع و فراموش کردن گذشته در محیطی تصادفی است که اتفاقات تصادفی بسیار میفتند و آخرین تجربه می‌تواند نمایانگر بخش کوچکی از احتمالات در آن محیط باشد.      \n",
    "    </b>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "از مشکلات روش بالا گفتیم. اما چه کنیم تا تابعی منطقی‌تر برای بروزرسانی داشته باشیم؟\n",
    "کافی است خاطره‌ای را که از قبل در ذهنمان نسبت به \n",
    "$Q(s,a)$\n",
    "داشتیم به صورت کامل فراموش نکنیم و بلکه آن را در خاطر خودمان داشته باشیم.\n",
    "به عبارت جدید بروزرسانی دقت کنید.\n",
    "$$Q_{new}(s,a) = \\alpha \\times [ R(s,a,s') + \\gamma \\times Val(s') ] + (1-\\alpha) \\times Q_{old}(s,a) $$\n",
    "یا به عبارت دیگر\n",
    "$$Q_{new}(s,a) = \\alpha \\times [ R(s,a,s') + \\gamma \\times max_{a'\\in A(s)} Q(s', a')] + (1-\\alpha) \\times Q_{old}(s,a)$$\n",
    "همانطور که می‌بینید در این جا ما \n",
    "    با یک ضریب \n",
    "    $\\alpha$\n",
    "    خاطرات جدیدمان را تاثیر می‌دهیم و با ضریب\n",
    "    $1- \\alpha$\n",
    "    خاطرات گذشته را به یاد می‌آوریم.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qlearning_update.py\n",
    "def qlearning_update(self, state, action, nextState, reward):\n",
    "    estimatedQ = self.alpha * (reward + self.discount * self.computeValueFromQValues(nextState)) + (1-self.alpha)*self.qValues[(state,action)]\n",
    "    self.qValues[(state,action)] = estimatedQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"random_learb\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "دقت کنید که پارامتر ورودی\n",
    "learningRate\n",
    "    پاس داده شده در واقع همان \n",
    "    alpha($\\alpha$)\n",
    "    ی بحث شده در بالاست.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -a q -k 30 -m --discount 0.9 --learningRate 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"random_learb\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "یادگیری تصادفی معهود:\n",
    " </font>\n",
    "همانطور که در زیر می‌بینیم، توانستیم با کمک همدیگر، الگوریتمی برای یادگیری\n",
    "Q-مقدار‌ها\n",
    "    به صورت کاملا اتوماتیک است. البته این الگوریتم، مشکلاتی دارد که راه‌حل‌هایی برای حل این مشکل‌ها ارائه می‌دهیم.\n",
    "    \n",
    "   \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -a q -k 100 --epsilon 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "محاسبه‌ی حرکات بهینه با توجه به Q-مقدار‌ها\n",
    ":\n",
    "</font>\n",
    "حالا که Q-مقدار‌ها\n",
    "    را داریم چگونه می‌توانیم با استفاده از آن‌ها، عمل بهینه را محاسبه کنیم و آن را انجام بدهیم؟\n",
    "  <div style=\"color:blue\"> \n",
    "      فرض کنید که در وضعیت \n",
    "      $s$\n",
    "      هستیم،\n",
    "      حالا باید بین اعمال مختلف و ممکن انتخاب کنیم.\n",
    "       مجموعه اعمال ممکن را \n",
    "      $A$\n",
    "      بنامید،\n",
    "      حال کافی است عمل\n",
    "      $a$\n",
    "      را انتخاب می‌کنیم که\n",
    "      $Q(s, a)$\n",
    "      برای آن بزرگتر مساوی\n",
    "      $Q(s, a')$\n",
    "      به ازای هر\n",
    "      $a' \\in A$\n",
    "      باشد. یعنی این که وقتی به ازای هر عمل، فهمیده‌ایم که امید ریاضی پاداش چقدر است، می‌توانیم عمل با بالاترین امید ریاضی را انتخاب کنیم.\n",
    "      \n",
    "      \n",
    "      \n",
    "  </div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"feature\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "پس کلا مساله حل شد و تمام؟\n",
    " </font>\n",
    " همانطور که در بخش قبل مشاهده کردید، ما الگوریتمی را پیدا کردیم که می‌تواند تمامی \n",
    " Q مقدار‌ها را به درستی یاد بگیرد.\n",
    " در ابتدا،‌شاید فکر کنید که مساله دیگر تمام شده است و این نهایت یادگیری تقویتی است ولی اینگونه نیست در واقع با یاد گرفتن تمام\n",
    "Q\n",
    "مقدار‌ها، ما می‌توانیم در هر مرحله به صورت کاملا دقیق مشخص کنیم که کدامین عمل در این وضعیت، بهترین انتخاب است یا در واقع بیش‌ترین امیدریاضی سود را به ما می‌رساند.\n",
    " پس مشکل چیست؟\n",
    " مشکل در واقع در سرعت این الگوریتم یادگیری است. چیزی که به طور مشخص وجود دارد این است که در بسیاری از مسایل دنیای واقعی، یا حتی در بسیاری از مسایل دنیای غیرواقعی فضای حالات به قدری بزرگ است که نمی‌توان برای تمامی حالات آن یک وضعیت در نظر گرفت و سپس در هر وضعیت آن اعمال مختلف را با یک الگوریتم تصادفی پیمایش کرد تا در عمل ببینیم که کدام حالت و کدام وضعیت می‌تواند بهتر باشد.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "جست و جوی بیش‌تر یا استفاده از آنچه داریم؟\n",
    "</font>\n",
    "همانطور که در بخش الگوریتم‌های تصادفی مشاهده کردید، اگرچه ما الگوریتمی تصادفی داریم که \n",
    "    Qمقدار‌ها\n",
    "    را یاد می‌گیرد اما حدس بزنید که الگوریتم ما چه اشکالی دارد؟\n",
    "    اشکال الگوریتم ما در واقع این است که اگر آن را در دنیای واقع اجرا کنیم،\n",
    "    خواهیم دید که در واقع برنامه‌ی ما به صورت تصادفی عمل می‌کند. البته علت آن هم واضح است و آن این است که در حال عمل تصادفی است تا بتواند \n",
    "    Q-مقدار‌ها\n",
    "    را یاد بگیرد اما هیچ کاری با این یادگیری انجام شده انجام نمی‌دهد.\n",
    "   بنابراین همانطور که احتمالا خودتان هم فکر می‌کنید باید پس از یک سری حرکات تصادفی حرکات تصادفی را کم و کنیم  و به آنچه آموخته‌ایم تکیه کنیم.\n",
    "در اینجا دو روش از راه‌های ادغام این روش به صورتی ریاضی با الگوریتم‌های یادگیری ما است.\n",
    "روش ما بسیار ساده است. ما در ابتدا یک \n",
    "$\\epsilon$\n",
    "را که عددی بین صفر و یک است انتخاب می‌کنیم.\n",
    "    برای مثال می‌توانیم فرض کنیم\n",
    "$\\epsilon = 0.1$.\n",
    "حال، در هر مرحله‌ی تصمیم‌گیری به احتمال\n",
    "$\\epsilon$\n",
    "    حرکتی کاملا تصادفی انجام می‌دهیم و به احتمال\n",
    "$1-\\epsilon$\n",
    "حرکتی را که با توجه به آنچه تا اینجا آموخته‌ایم، بهینه است، انجام می‌دهیم.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -a q -k 100 --epsilon 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "همانطور که در مثال بالا مشاهده می‌کنید پس از مدتی، راه برای کسب امتیاز بهینه مشخص می‌شود و بنابراین دیگر اپسیلون با آن مقدار بالا چندان منطقی به نظر نمی‌رسد. در این جا است که از روشی استفاده می‌کنند که آرام آرام مقدار اپسیلون را کاهش می‌دهد تا پس از مدتی کم کم جست و جو متوقف و راه بهینه را دنبال کنیم. اما دقت کنید که چنین کاری باعث میشود تا  دیگر از اینکه \n",
    "    Q\n",
    "    مقدار‌های صحیح را به دست می آوریم مطمین نباشیم.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qlearning_update.py\n",
    "def qlearning_update(self, state, action, nextState, reward):\n",
    "    self.alpha = 0.5\n",
    "    estimatedQ = self.alpha * (reward + self.discount * self.computeValueFromQValues(nextState)) + (1-self.alpha)*self.qValues[(state,action)]\n",
    "    self.qValues[(state,action)] = estimatedQ\n",
    "    self.epsilon = (self.epsilon)*0.999\n",
    "    print('epsilon is', self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -a q -k 100 --epsilon 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    حتی می‌توانیم اعتمادمان نسبت به آنچه یاد گرفته‌ایم را بیش‌تر هم بکنیم  و بیشتر در یادگیری به آن اعتماد کنیم.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qlearning_update.py\n",
    "def qlearning_update(self, state, action, nextState, reward):\n",
    "    estimatedQ = self.alpha * (reward + self.discount * self.computeValueFromQValues(nextState)) + (1-self.alpha)*self.qValues[(state,action)]\n",
    "    self.qValues[(state,action)] = estimatedQ\n",
    "    self.epsilon = (self.epsilon)*0.999\n",
    "    self.alpha = (self.alpha*999+1)/1000\n",
    "    print(self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -a q -k 100 --epsilon 0.99 --learningRate 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "می‌توانیم آنچه نوشته‌ایم را بر روی محیط بزرگتری امتحان کنیم:\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -a q -k 50 -n 0 -g BridgeGrid --epsilon 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "در نهایت، تابع را به آنچه در ابتدا بود برمی‌گردانیم! می‌توانید هرچقدر دلتان می‌خواهد آن را تغییر دهید:\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qlearning_update.py\n",
    "def qlearning_update(self, state, action, nextState, reward):\n",
    "    estimatedQ = self.alpha * (reward + self.discount * self.computeValueFromQValues(nextState)) + (1-self.alpha)*self.qValues[(state,action)]\n",
    "    self.qValues[(state,action)] = estimatedQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "جدی جدی وقت یادگیری یک بازیه!\n",
    "</font>\n",
    "در این قسمت می‌خواهیم با استفاده از الگوریتم‌هایی که یاد گرفتیم برای بازی\n",
    "pacman\n",
    "یک الگوریتم بنویسیم که بازی بهینه را یاد بگیرد. در ابتدا یک بار بازی پک من را اجرا کنیم.\n",
    "    دستور زیر در ابتدا دو هزار بار بازی را برای یادگیری پک‌من اجرا می‌کند و سپس  اپسیلون و آلفا را صفر کرده و ده بار از بازی را برای ما نمایش می‌دهد. ببینیم:\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pacman.py -p PacmanQAgent -x 2000 -n 2010 -l smallGrid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "بیایید همین بازی را بر روی نقشه‌ای کمی بزرگتر انجام دهیم:\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pacman.py -p PacmanQAgent -x 2000 -n 2010 -l mediumGrid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "اشکال کجاست؟\n",
    "</font>\n",
    "فکر می‌کنید اشکال کار کجاست و چرا الگوریتم ما اینگونه ضعیف عمل می‌کند؟ در واقع  علت بزرگی استیت‌ها و تعداد حالات بسیار زیادی بازی پک‌من است. یکی از راه‌حل‌های این موضوع همانطور که در بخش‌های قبلی همین یادداشت گفته شد، کاهش تعداد وضعیت‌هاست. اما این‌جا به بررسی یک راه‌حل بسیار جالب برای حل مساله‌ي پک‌من می‌پردازیم. ما می‌خواهیم\n",
    "    $Q(s, a)$\n",
    "    را محاسبه کنیم و در واقع تمام این سختی‌هایی که متحمل می‌شویم برای محاسبه‌ی همین امر است. حال فرض کنید که \n",
    "    $$Q(s, a) = \\sum_{i}^{n} fـ{i}(s, a)\\times w_{i}$$\n",
    "    که در این فرمول \n",
    "    $f_{i}(s, a)$\n",
    "    تابعی است که وضعیت ورودی و عمل را می‌گیرد و یک عدد را بر می‌گرداند. در واقع این عدد یک ویژگی از بازی است.\n",
    "    حال پس از هر مرحله تفاوت بین آنچه رخ می‌دهد و آنچه انتظار داریم رخ بدهد را مشاهده می‌کنیم و با توجه به آن ضرایب\n",
    "    ، یعنی \n",
    "    $w_{i}$\n",
    "    ها را به گونه‌ای تغییر می‌دهیم که تخمین بهتری از\n",
    "    $Q(s, a)$\n",
    "    ها بزنند.\n",
    "    در واقع داریم که:\n",
    "    $$difference = (reward+\\gamma \\times \\underset{a'}{max} Q(s', a')) - Q(s, a)$$\n",
    "    $$w_{i} \\leftarrow w_{i}+\\alpha \\times difference \\times f_{i}(s, a)$$\n",
    "    احتمالا مساله‌ی اصلی در چنین راه‌حلی این است که چه ویژگی‌هایی را استخراج کنیم.\n",
    "    برای یک پیاده‌سازی ساده می‌توانید فایل\n",
    "    featureExtracotrs.py\n",
    "    را مشاهده کنید:\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile featureExtractors.py\n",
    "\"Feature extractors for Pacman game states\"\n",
    "\n",
    "from game import Directions, Actions\n",
    "import util\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def getFeatures(self, state, action):\n",
    "        \"\"\"\n",
    "          Returns a dict from features to counts\n",
    "          Usually, the count will just be 1.0 for\n",
    "          indicator functions.\n",
    "        \"\"\"\n",
    "        util.raiseNotDefined()\n",
    "\n",
    "class IdentityExtractor(FeatureExtractor):\n",
    "    def getFeatures(self, state, action):\n",
    "        feats = util.Counter()\n",
    "        feats[(state,action)] = 1.0\n",
    "        return feats\n",
    "\n",
    "class CoordinateExtractor(FeatureExtractor):\n",
    "    def getFeatures(self, state, action):\n",
    "        feats = util.Counter()\n",
    "        feats[state] = 1.0\n",
    "        feats['x=%d' % state[0]] = 1.0\n",
    "        feats['y=%d' % state[0]] = 1.0\n",
    "        feats['action=%s' % action] = 1.0\n",
    "        return feats\n",
    "\n",
    "def closestFood(pos, food, walls):\n",
    "    \"\"\"\n",
    "    closestFood -- this is similar to the function that we have\n",
    "    worked on in the search project; here its all in one place\n",
    "    \"\"\"\n",
    "    fringe = [(pos[0], pos[1], 0)]\n",
    "    expanded = set()\n",
    "    while fringe:\n",
    "        pos_x, pos_y, dist = fringe.pop(0)\n",
    "        if (pos_x, pos_y) in expanded:\n",
    "            continue\n",
    "        expanded.add((pos_x, pos_y))\n",
    "        # if we find a food at this location then exit\n",
    "        if food[pos_x][pos_y]:\n",
    "            return dist\n",
    "        # otherwise spread out from the location to its neighbours\n",
    "        nbrs = Actions.getLegalNeighbors((pos_x, pos_y), walls)\n",
    "        for nbr_x, nbr_y in nbrs:\n",
    "            fringe.append((nbr_x, nbr_y, dist+1))\n",
    "    # no food found\n",
    "    return None\n",
    "\n",
    "class SimpleExtractor(FeatureExtractor):\n",
    "    \"\"\"\n",
    "    Returns simple features for a basic reflex Pacman:\n",
    "    - whether food will be eaten\n",
    "    - how far away the next food is\n",
    "    - whether a ghost collision is imminent\n",
    "    - whether a ghost is one step away\n",
    "    \"\"\"\n",
    "\n",
    "    def getFeatures(self, state, action):\n",
    "        # extract the grid of food and wall locations and get the ghost locations\n",
    "        food = state.getFood()\n",
    "        walls = state.getWalls()\n",
    "        ghosts = state.getGhostPositions()\n",
    "\n",
    "        features = util.Counter()\n",
    "\n",
    "        features[\"bias\"] = 1.0\n",
    "\n",
    "        # compute the location of pacman after he takes the action\n",
    "        x, y = state.getPacmanPosition()\n",
    "        dx, dy = Actions.directionToVector(action)\n",
    "        next_x, next_y = int(x + dx), int(y + dy)\n",
    "\n",
    "        # count the number of ghosts 1-step away\n",
    "        features[\"#-of-ghosts-1-step-away\"] = sum((next_x, next_y) in Actions.getLegalNeighbors(g, walls) for g in ghosts)\n",
    "\n",
    "        # if there is no danger of ghosts then add the food feature\n",
    "        if not features[\"#-of-ghosts-1-step-away\"] and food[next_x][next_y]:\n",
    "            features[\"eats-food\"] = 1.0\n",
    "\n",
    "        dist = closestFood((next_x, next_y), food, walls)\n",
    "        if dist is not None:\n",
    "            # make the distance a number less than one otherwise the update\n",
    "            # will diverge wildly\n",
    "            features[\"closest-food\"] = float(dist) / (walls.width * walls.height)\n",
    "        features.divideAll(10.0)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qlearning_update.py\n",
    "def qlearning_update(self, state, action, nextState, reward):\n",
    "    difference = reward + self.discount * self.getValue(nextState) - self.getQValue(state, action)\n",
    "    features = self.featExtractor.getFeatures(state, action)\n",
    "    for key in features.keys():\n",
    "        self.weights[key] = self.weights[key] + self.alpha * difference * features[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pacman.py -p ApproximateQAgent -a extractor=SimpleExtractor -x 50 -n 60 -l mediumGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pacman.py -p ApproximateQAgent -a extractor=SimpleExtractor -x 50 -n 60 -l mediumClassic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "ایده‌هایی متفاوت‌تر!\n",
    "</font>\n",
    "همانطور که احتمالا می‌دانید، آنچه در این یادداشت باهم دیگر خواندیم و دیدیم، در واقع بخش بسیار کوچکی از یادگیری عمق بود.\n",
    "برای یاد گرفتن استراتژی بهینه می‌توان الگوریتم‌های بسیار زیادی را پیشنهاد کرد.\n",
    "Q-learning\n",
    "، که در واقع روشی بسیار مستقیم است تنها یکی از آن‌هاست.\n",
    "از طرف دیگر، روش‌های بسیار زیادی را می‌توان برای استخراج ویژگی‌ها از یک بازی استفاده کرد.\n",
    "    همانطور که در آخر این جزوه دیدیم، به صورت دستی و طبق شهود انسان‌ها ویژگی‌هایی را از بازی پک من استخراج کردیم که به نظر خودمان استراتژی‌های مفیدی بودند.\n",
    "    اما این چیزی نیست که خیلی جالب باشد زیرا در نهایت متکی به هوش انسانی شدیم.\n",
    "    برای رفع همین مشکل، همانطور که قبلا هم اشاره کردیم، می‌توان از روش‌های مختلفی مثل شبکه‌های عصبی بهره جست تا برای ما نمایش‌های مفیدی از وضعیت بازی به صورت خودکار استخراج کنند. \n",
    "    حتی در مورد جزییات می‌توان بسیار فکر کرد و خواند و جست و جو کرد.\n",
    "    برای مثال همانطور که در همین یادداشت مشاهده کردید مقادیر آلفا و گاما تاثیرات بسیاری بر یهینه بودن الگوریتم خواهند داشت.\n",
    "    برای دسترسی به منابع بیشتر و کامل‌تر می‌توانید از منبع زیر که مربوط به درس هوش مصنوعی دانشگاه برکلی است استفاده کنید:\n",
    "    <ul>\n",
    "            <li>\n",
    "                <a href=\"http://ai.berkeley.edu\">AI-Berkeley</a>                                     \n",
    "            </li>\n",
    "        </ul>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "سخن پایانی\n",
    "</font>\n",
    "از صبر و حوصله‌ی شما بسیار سپاس‌گزاریم! امیدواریم محتواهای علمی مفید بوده باشند و مشکلات این محتواها را بر ما ببخشید.\n",
    "    در صورت مشاهده هرگونه ایرادی اعم از علمی و نگارشی و ... حتما به تیم رویداد هوش مصنوعی اطلاع دهید تا بتوانیم محتواها را بهبود ببخشیم.\n",
    "    همچنین اگر انتقاد و پیشنهادی در رابطه با محتواها داشتید حتما با ما در میان بذارید.\n",
    "    <br>\n",
    "    راه ارتباطی: <a href=\"mailto:aichallenge@sharif.edu\">aichallenge@sharif.edu</a>\n",
    "</font>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
