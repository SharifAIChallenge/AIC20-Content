{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<div align=center>\n",
    "\t\t\t<img src=\"images/markov.jpg\" style=\"width: 100%\">\n",
    "\t\t</div>\n",
    "\t\t<style type=\"text/css\" scoped>\n",
    "        p{\n",
    "        border: 1px solid #a2a9b1;background-color: #f8f9fa;display: inline-block;\n",
    "        };\n",
    "        </style>\n",
    "\t\t<div>\n",
    "\t\t\t<h3>فهرست مطالب</h3>\n",
    "\t\t\t<ul style=\"margin-right: 0;\">\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#INTRO\">\n",
    "                        مقدمه \n",
    "                    </a>                  \n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#MDP\">\n",
    "                        فرایند‌های تصمیم‌گیری مارکوف\n",
    "                    </a>                   \n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#EXM\">\n",
    "                        یک مثال\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#FOR\">\n",
    "                    صورت بندی فرایند‌های تصمیم‌گیری مارکوف\n",
    "                    </a>\n",
    "\t\t\t\t</li>   \n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#REW\">\n",
    "                    دربارهٔ پاداش\n",
    "                    </a>\n",
    "\t\t\t\t</li> \n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#SOL\">\n",
    "                    به دنبال استراتژی بهینه\n",
    "                    </a>\n",
    "\t\t\t\t</li> \n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#PVSL\">\n",
    "                    برنامه‌ریزی و یادگیری\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#REF\">\n",
    "                    منابع\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "        </div>\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"INTRO\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "هیچ قطعیتی در کار نیست! \n",
    "        </font>\n",
    "        <p></p>\n",
    "        تا به اینجای کار با محیط‌هایی سر و کار داشته‌ایم که نتیجهٔ انجام کنش‌های مختلف در آن‌ها را به صورت قطعی می‌دانستیم. در چنین شرایطی تنها کافی بود دنباله‌ای از کنش‌های مناسب را بیابیم که ما را به نقطهٔ مطلوب مورد نظر برسانند. اما در دنیای واقع بیشتر با محیط‌هایی مواجه هستیم که عنصر احتمال در آن‌ها نقش کلیدی ایفا می‌کند و پیشامدهای محیط به صورت احتمالی مشخص می‌شوند. احتمالی بودن محیط می‌تواند خاصیت ذاتی آن باشد؛ مانند یک بازی که در آن کنش‌ها بر حسب انداختن تاس مشخص می‌شوند، و یا می‌تواند ناشی از اطلاعات کم ما دربارهٔ عناصر تعیین کننده در محیط باشد که به صورت احتمالی بودن رخدادها بروز پیدا می‌کند. مثلا این‌که امروز در تهران باران بیاید یا نه امریست که به متغیرهای پیچیدهٔ زیادی بستگی دارد که با دانستن آن‌ها می‌توان بارش یا عدم بارش را تشخیص داد اما با توجه به پیچیدگی بسیار بالای آن چنین کاری امکان پذیر نیست و می‌توانیم به جای آن با توجه به تجربه‌های پیشین آن را یک احتمال در نظر بگیریم.  \n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"MDP\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "تصمیم مارکوف! \n",
    "        </font>\n",
    "        <p></p>\n",
    "        در چنین شرایطی از آن‌جا که نتیجهٔ کنش‌ها به صورت قطعی مشخص نیست پیدا کردن یک دنباله از کنش‌ها برای بهینه عمل کردن در محیط کافی نیست. چرا که با توجه به خاصیت احتمالی محیط باید انتظار قرار گرفتن در وضعیت‌های مختلف به دنبال یک کنش را داشته باشیم. برای همین در چنین مسائلی به یک استراتژی نیاز داریم که به ازای قرار گرفتن در هر وضعیت کنش مطلوب برای آن وضعیت را به ما نشان دهد. در این صورت دیگر نگران عدم قطعیت موجود در محیط نخواهیم بود، چرا که برای قرار گرفتن در هر وضعیتی از محیط می‌دانیم چه کنشی بهینه است و کاری بیش از انجام آن هم از دستمان بر نمی‌آید. فرایند‌های تصمیم‌گیری مارکوف یا MDP یک چارچوب برای مدل‌سازی مسائلی که در آن‌ها در هر لحظه وضعیت فعلی اهمیت دارد و اتفاقات پیش رو با دانستن وضعیت فعلی مستقل از وضعیت‌های پیشین هستند ارائه می‌دهد که سعی می‌کنیم با معرفی آن به سمت حل چنین مسائلی حرکت کنیم! \n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"EXM\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "مثال - دنیای جدولی \n",
    "        </font>\n",
    "        <p></p>\n",
    "         بیایید قبل از این که سراغ صورت بندی فرایند‌های تصمیم‌گیری مارکوف برویم یک محیط را به عنوان مثال در نظر بگیریم تا در ادامه هم ناظر به آن صحبت کنیم. برای این منظور از محیط «دنیای جدولی» پروژهٔ دانشگاه برکلی استفاده می‌کنیم.\n",
    "        در این محیط یک جدول داریم که تعدادی از خانه‌های آن مانع هستند، در یک خانه الماس و در یک خانه هم آتش وجود دارد. عامل ما در ابتدا در یکی از خانه‌های جدول قرار دارد و می‌تواند در هر حرکت به یکی از خانه‌های مجاور برود. البته اگر یک کنش منجر به رفتن به خانهٔ مانع یا خارج از جدول شود جای عامل تغییری نمی‌کند. عامل دوست دارد با انجام کنش‌های مناسب به خانهٔ الماس که بسیار مورد علاقه‌اش هست برود و مواظب باشد به خانهٔ دارای آتش نرود. در صورت رفتن به هر کدام از این دو خانه تنها می‌تواند به خانهٔ پایان بازی برود و بازی پایان می‌پذیرد.\n",
    "        <img src='images/gridworld.png'/>\n",
    "        خب احتمالا می‌گویید این که کاری ندارد! با استفاده از الگوریتم‌هایی که در بخش جستجو یاد گرفتیم به دنبال یک دنباله از کنش‌ها می‌گردیم که ما را از خانهٔ ابتدایی بدون گذر از موانع و خانهٔ آتش به خانهٔ الماس برساند. تازه می‌توانیم کوتاه‌ترین دنباله را هم پیدا کنیم که عامل‌مان کمتر به زحمت بیافتد. حق با شماست! اما به این شرط که هر گاه عامل قصد کرد به یک خانهٔ مجاور برود و کنش رفتن به آن خانه را انجام داد بدون دردسر به آن خانه برود. اما «دنیای جدولی» این شکلی نیست! بلکه یک خاصیت احتمالی دارد. به این شکل که هر بار که عامل رو به خانهٔ مجاورش کند و کنش رفتن به آن خانه را انجام دهد، به احتمال ۰.۸ به همان خانه می‌رود، به احتمال ۰.۱ ابتدا ۹۰ درجه به راست می‌پیچد و به خانهٔ روبه‌رویی‌اش می‌رود و به احتمال ۰.۱ هم ابتدا ۹۰ درجه به چپ می‌پیچد و به خانهٔ روبه‌رویی‌اش می‌رود. شکل زیر تفاوت خروجی کنش‌ها در «دنیای جدولی» احتمالی ما و دنیای جدولی‌ای که کنش‌ها نتیجهٔ قطعی در پی داشته باشند را نشان می‌دهد. همین تفاوت باعث می‌شود داستان به کلی تغییر کند و لازم شود آمادگی قرار گرفتن در هر وضعیتی و گرفتن تصمیم بهینه در آن را داشته باشیم.:\n",
    "        <img src='images/gridworld_actions.png'>\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"FOR\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "صورت بندی MDP\n",
    "        </font>\n",
    "        <p></p>\n",
    "        یک MDP به وسیلهٔ عناصر زیر صورت‌بندی می‌شود:\n",
    "        <ul style=\"color:darkblue\">\n",
    "            <li>\n",
    "                $S$: مجموعهٔ وضعیت‌ها\n",
    "            </li>\n",
    "            <li>\n",
    "                $A$: مجموعهٔ کنش‌ها\n",
    "            </li>\n",
    "            <li>\n",
    "                $T(s, a, s')$: تابع گذار\n",
    "                        - با فرض قرار داشتن در وضعیت\n",
    "                        $s$،\n",
    "                        احتمال قرار گرفتن در وضعیت \n",
    "                        $s'$\n",
    "                        بعد از انجام کنش \n",
    "                        $a$ \n",
    "                        را مشخص می‌کند.\n",
    "            </li>\n",
    "            <li>\n",
    "                $R(s, a, s')$: تابع پاداش               \n",
    "                - با فرض قرار داشتن در وضعیت\n",
    "                $s$،\n",
    "                پاداش دریافتی در صورت قرار گرفتن در وضعیت\n",
    "                $s'$ \n",
    "                بعد از انجام کنش\n",
    "                $a$                       \n",
    "                را مشخص می‌کند.\n",
    "            </li>\n",
    "            <li>\n",
    "                وضعیت ابتدایی\n",
    "            </li>\n",
    "        </ul>\n",
    "        مجموعهٔ وضعیت‌ها و کنش‌ها مثل قبل برای صورت‌بندی محیط لازم هستند. در «دنیای جدولی» تنها چیزی که تغییر می‌کند مکان عامل است؛ بنابراین به ازای هر خانه از جدول که عامل ممکن است در آن قرار بگیرد یک وضعیت تعریف می‌کنیم. کنش‌ها هم به ازای هر خانهٔ خالی رفتن به یکی از ۴ خانهٔ مجاور هستند.\n",
    "        <p></p>\n",
    "        علاوه بر وضعیت‌ها و کنش‌ها تابع\n",
    "        T\n",
    "        را داریم که خاصیت احتمالی محیط را مدل می‌کند و احتمال قرار گرفتن در هر وضعیتی به ازای انجام یک کنش خاص در یک وضعیت خاص را نشان می‌دهد. برای مثال در محیط «دنیای جدولی» عامل به ازای انجام یک کنش خاص در یک وضعیت خاص ممکن است با احتمال‌های مختلف به حداکثر سه خانهٔ متفاوت برود.\n",
    "        <p></p>\n",
    "        عنصر بعدی تابع \n",
    "        R\n",
    "        است. این تابع میزان مطلوبیت اتفاقاتی که در محیط می‌افتد را مدل می‌کند و پاداش قرار گرفتن در هر وضعیتی به ازای انجام یک کنش خاص در یک وضعیت خاص را نشان می‌دهد. برای مثال در «دنیای جدولی» می‌توان پاداش قرار گرفتن در خانهٔ الماس را یک عدد مثبت بزرگ و پاداش قرار گرفتن در خانهٔ آتش را یک عدد منفی بزرگ نظر گرفت. این تابع راهنمای ما برای سنجش مطلوبیت کنش‌هاییست که انجام می‌دهیم و در واقع وقتی می‌گوییم عامل هوشمندمان \n",
    "        <b>«بهینه»</b>\n",
    "        است که کنش‌هایی که انجام می‌دهد باعث بیشینه شدن امید ریاضی پاداش‌های دریافتی‌اش در طول زمان شود.  \n",
    "        <p></p>\n",
    "        با داشتن این عناصر می‌خواهیم یک استراتژی به صورت تابعی از وضعیت‌ها به کنش‌ها مانند\n",
    "        $\\pi^*: S \\rightarrow A$\n",
    "        بیابیم که بهینه باشد (عامل پیرو آن بهینه باشد). \n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br/>\n",
    "<div id=\"REW\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "دربارهٔ پاداش\n",
    "        </font>\n",
    "        <p></p>\n",
    "       می‌دانیم که پاداش به دست آمده هر چه بیشتر باشد بهتر است. هم‌چنین پاداشی که زودتر به دستمان برسد را بیشتر دوست داریم. چون باعث می‌شود تنها این که راهی برای رسیدن به پاداش وجود دارد عاملمان را راضی نکند و آن را به سمت کسب پاداش هدایت کند. در نتیجه در کنار بیشینه کردن پاداش، زود به دست آمدن آن هم اهمیت پیدا می‌کند. برای این که عاملمان را به زودتر کسب کردن پاداش تشویق کنیم می‌توانیم از رفتن کاستن پاداش استفاده کنیم. به این شکل که به پاداش‌هایی که در یک قدمی عامل هستند و در نتیجهٔ حرکت بعدی کسب می‌شوند ضریب ۱، به پاداش‌هایی که در دو قدمی هستند ضریب \n",
    "        &gamma;\n",
    "        و به طور کلی به پاداش‌هایی که در \n",
    "        k\n",
    "        قدمی هستند ضریب\n",
    "        &gamma;<sup>k</sup>\n",
    "        می‌دهیم.\n",
    "        این کار باعث می‌شود عامل به پاداش‌های نزدیک‌تر که نقدتر هستند توجه بیشتری کند. اما روش کاستن پاداش‌ها یک فایدهٔ اساسی دیگر هم دارد. این حرکت باعث می‌شود الگوریتم ما همگرا شود و به جای سر و کار داشتن با دنباله‌هایی از پاداش‌ها که مجموعشان ممکن است نامتناهی باشد با مجموع پاداش‌های متناهی کار داشته باشد. این خاصیت بسیار مهمیست که محاسبهٔ پاداش بهینه به ازای آغاز از هر وضعیت و در نتیجه مقایسهٔ پاداش‌های مختلف را امکان‌پذیر می‌کند. هم‌چنین این روش پاداش‌دهی ویژگی\n",
    "        stationary preferences\n",
    "        که یک ویژگی لازم برای نوع پاداش‌دهی است را تامین می‌کند که می‌توانید خودتان دربارهٔ آن بیشتر مطالعه کنید.\n",
    "        <img src='images/discount.png' width=400>\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"SOL\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "به دنبال استراتژی بهینه \n",
    "        </font>\n",
    "        <p></p>\n",
    "         حال چطور می‌توانیم به ازای هر وضعیت کنش بهینه برای آن را پیدا کنیم؟ \n",
    "        <br>\n",
    "در گام اول سعی می‌کنیم مسئله را دقیق‌تر مدل کنیم تا بعد از آن به حل آن بپردازیم. بیاید کمیت‌های زیر را تعریف کنیم:\n",
    "        <br>\n",
    "        <ul style=\"color:darkblue\">\n",
    "            <li>\n",
    "                $V^*(s)$: پاداش مورد انتظار در صورت عملکرد بهینه با شروع از وضعیت s\n",
    "            </li>\n",
    "            <li>\n",
    "                $Q^*(s, a)$: پاداش مورد انتظار در صورت عملکرد بهینه با شروع از وضعیت s و انجام کنش ابتدایی a\n",
    "            </li>\n",
    "            <li>\n",
    "                $\\pi^*(s)$: کنش بهینه در وضعیت s                   \n",
    "            </li>\n",
    "        </ul>\n",
    "        مطلوب نهایی ما تابع \n",
    "        $\\pi^*$\n",
    "        است. طبق تعریف‌های فوق کنش بهینه در وضعیت \n",
    "        s\n",
    "        کنشی مثل \n",
    "        a\n",
    "        است که مقدار\n",
    "        $Q^*(s, a)$\n",
    "        را بیشینه کند. بنابراین داریم:\n",
    "        <p></p>\n",
    "        <center>\n",
    "        $\\pi^*(s) = \\arg\\max_a Q^*(s, a)$\n",
    "        </center>\n",
    "        <p></p>\n",
    "        پس با به دست آوردن \n",
    "        $Q^*$\n",
    "        می‌توانیم \n",
    "        $\\pi^*$\n",
    "        را به دست آوریم. حال چطور \n",
    "        $Q^*$\n",
    "        را به دست آوریم؟\n",
    "        <br>\n",
    "        تابع \n",
    "        $T$\n",
    "        در \n",
    "        MDP\n",
    "        احتمال قرار گرفتن در هر وضعیتی به ازای انجام یک کنش خاص در یک وضعیت را در اختیارمان قرار می‌دهد. پس امید ریاضی یا پاداش مورد انتظار در صورت انجام کنش \n",
    "        a\n",
    "        در وضعیت \n",
    "        s\n",
    "        متناسب با احتمالی که \n",
    "        T\n",
    "        برای رفتن به وضعیت‌ها ارائه می‌کند از روی مقدار بهینهٔ سایر وضعیت‌ها به دست می‌آید. به عبارت دقیق‌تر داریم:\n",
    "        <p></p>\n",
    "        <center>\n",
    "        $Q^*(s, a) = \\sum_{s\\prime} T(s, a, s\\prime)[R(s, a, s\\prime) + \\gamma V^*(s\\prime)]$\n",
    "        </center>\n",
    "        <p></p>\n",
    "        که در آن\n",
    "        $\\gamma$\n",
    "        ضریب کاستن پاداش و \n",
    "        $R(s, a, s\\prime)$\n",
    "        پاداش رفتن به وضعیت\n",
    "        $s\\prime$\n",
    "        در ازای انجام کنش\n",
    "        a\n",
    "        در وضعیت\n",
    "        s\n",
    "        است.\n",
    "        <p></p>\n",
    "        پس توانستیم Q-مقدار‌ها\n",
    "        را هم بر اساس V-مقدار‌ها\n",
    "        به دست آوریم. حال چگونه می‌توانیم مقدار V\n",
    "        به ازای یک وضعیت مثل s\n",
    "        را به دست آوریم؟\n",
    "        با استفاده از Q-مقدار‌ها\n",
    "        این کار ساده است. کافیست ببینیم انجام کدام کنش در s\n",
    "        بهینه است و مقدار V(s)\n",
    "        را برابر با آن Q-مقدار‌\n",
    "        قرار دهیم. به این شکل:\n",
    "        <p></p>\n",
    "        <center>\n",
    "        $V^*(s) = \\max_a Q^*(s, a)$\n",
    "        </center>\n",
    "        <p></p>\n",
    "        اما پیش از این Q-مقدار‌ها\n",
    "        را بر اساس V-مقدارها\n",
    "        حساب کرده بودیم. بنابراین مقداری که بر حسب V-مقدارها\n",
    "        برای \n",
    "        $Q^*(s, a)$\n",
    "        به دست آورده بودیم را در عبارت بالا جایگزین می‌کنیم. در نتیجه خواهیم داشت:\n",
    "        <p></p>\n",
    "        <center>\n",
    "        $V^*(s) = \\max_a \\sum_{s\\prime} T(s, a, s\\prime)[R(s, a, s\\prime) + \\gamma V^*(s\\prime)]$\n",
    "        </center>\n",
    "        <p></p>\n",
    "        همان‌طور که می‌بینید توانستیم یک V-مقدار\n",
    "        را بر اساس سایر \n",
    "        V-مقدارها\n",
    "        بنویسیم. \n",
    "        <br>\n",
    "        به معادلاتی که تا کنون به دست آورده‌ایم\n",
    "        Bellman Equations\n",
    "        می‌گویند. اگر بتوانیم این معادلات را حل کنیم و V-مقدارها\n",
    "        و در نتیجهٔ آن Q-مقدارها \n",
    "        را محاسبه کنیم، می‌توانیم تابع \n",
    "        $\\pi^*$\n",
    "        که به دنبالش بودیم را به دست آوریم.\n",
    "        برای حل معادله‌های فوق روش‌های جالبی ارائه شده است که شرح آن‌ها در این دفترچه نیاورده‌ایم، اما اگر علاقه‌مند هستید می‌توانید با جستجوی کلیدواژه‌های \n",
    "        Value Iteration\n",
    "        و\n",
    "        Policy Iteration\n",
    "        دربارهٔ این روش‌ها بیاموزید. دلیل آن‌ که روش‌های حل \n",
    "        MDP\n",
    "        را شرح نداده‌ایم آن است که در اغلب مسائلی که با آن‌ها سر و کار داریم توابع T و R\n",
    "        ناشناخته هستند و وقتی اطلاعی از مقادیر آن‌ها نداشته باشیم نمی‌توانیم MDP\n",
    "        مورد نظر را حل کنیم. در چنین شرایطی باید سراغ «یادگیری» از طریق تعامل با محیط برویم. در دفترچهٔ بعدی سعی می‌کنیم با استفاده از مفاهیمی که تا کنون آموخته‌ایم روش یادگیری تقویتی را فرا بگیریم.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"PVSL\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "    برنامه‌ریزی و یادگیری\n",
    "    </font>     \n",
    "        <p></p>\n",
    "        حل مسئلهٔ MDP\n",
    "        یک برنامه‌ریزی پیشین یا آفلاین است. به این شکل که با داشتن جزئیات MDP\n",
    "        کمیت‌های مطلوب را یک بار محاسبه می‌کنیم و سپس طبق دستورالعمل بهینهٔ به دست آمده عمل می‌کنیم. در واقع نیازی به تعامل با محیط نداریم و به صورت آفلاین همهٔ محاسبات و نتیجه‌گیری‌ها را انجام می‌دهیم. اما فرض کنید در ابتدای کار از جزئیات و شکل رفتار MDP بی‌خبر باشیم.\n",
    "        در چنین شرایطی چه کاری از دستمان برمی‌آید؟ همان‌طور که احتمالا حدس می‌زنید در چنین موقعیتی ناگزیر از تعامل با محیط هستیم. همان‌طور که یک بچه در ابتدا از محیط آگاهی چندانی ندارد و باید با آزمون و خطا راه رفتن را بیاموزد! \n",
    "        بنابراین مسئله کاملا دگرگون می‌شود و عامل ما به جای برنامه‌ریزی در واقع باید یاد بگیرد. در این مسئلهٔ جدید با مفاهیم و چالش‌های تازه‌ای از جمله موارد زیر رو به رو هستیم:\n",
    "        <ul>\n",
    "            <li>\n",
    "                شناسایی: با در محیط با انجام کنش‌های ناشناخته در مورد ویژگی‌های محیط اطلاعات به دست بیاوریم.\n",
    "            </li>\n",
    "            <li>\n",
    "                بهره‌برداری: در کنار فرایند شناسایی باید از اطلاعات به دست آمده برای بهبود عملکردمان استفاده کنیم.\n",
    "            </li>\n",
    "            <li>\n",
    "                افسوس: در این مسئلهٔ جدید واقعا داریم برای یادگیری با محیط تعامل می‌کنیم و این که این یادگیری در نتیجهٔ چه میزان آزمون و خطا و هزینه‌ای(که آن را میزان افسوس می‌نامیم) اهمیت بالایی دارد.\n",
    "            </li>\n",
    "        </ul>\n",
    "        چالش‌های بالا و چالش‌های دیگر این مسئله می‌توانند مسئلهٔ «یادگیری» را بسیار سخت‌تر از مسئلهٔ حل کردن MDP\n",
    "        کنند. در دفترچهٔ بعدی خواهید دید که چطور می‌توانیم تا حدی از پس چالش‌های موجود بربیاییم و یک عامل هوش مصنوعی بسازیم که واقعا یاد می‌گیرد! \n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br/>\n",
    "<div id=\"REF\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "منابع\n",
    "        </font>\n",
    "\t\t<hr>       \n",
    "        <ul>\n",
    "            <li>\n",
    "                <a href=\"http://ai.berkeley.edu/home.html\">AI-Berkeley</a>                                     \n",
    "            </li>\n",
    "            <li>\n",
    "                 <a href=\"http://ce.sharif.edu/courses/97-98/1/ce417-1/\">CE417-1</a> \n",
    "            </li>   \n",
    "        </ul>\n",
    "\t</font>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
